{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD4 - Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Deep Q-Learning \n",
    "\n",
    "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
    "\n",
    "The parameters of the neural network are denoted by $\\theta$. \n",
    "*   As input, the network takes a state $s$,\n",
    "*   As output, the network returns $Q_\\theta [a | s] = Q_\\theta (s,a) = Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
    "\n",
    "\n",
    "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a) \\simeq Q_{\\theta^*} (s,a)$. \n",
    "\n",
    "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
    "\n",
    "2.  Choose action $a_t = \\arg\\max_a Q_\\theta(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
    "\n",
    "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
    "\n",
    "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
    "\n",
    "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
    "\\left[\n",
    "Q(s_i, a_i, \\theta) -  y_i\n",
    "\\right]^2\n",
    "$$\n",
    "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
    "\n",
    "$$\n",
    "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
    "$$\n",
    "\n",
    "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
    "$$\n",
    "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "where $\\eta$ is the optimization learning rate. \n",
    "\n",
    "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
    "\n",
    "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "# from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python --version = 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n",
      "torch.__version__ = 1.8.1+cu102\n",
      "np.__version__ = 1.20.2\n",
      "gym.__version__ = 0.18.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"python --version = {sys.version}\")\n",
    "print(f\"torch.__version__ = {torch.__version__}\")\n",
    "print(f\"np.__version__ = {np.__version__}\")\n",
    "print(f\"gym.__version__ = {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch 101\n",
    "\n",
    ">\"The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. \n",
    "[...] provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\" \n",
    "[PyTorch](https://pytorch.org/docs/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_torch is of type <class 'torch.Tensor'>\n",
      "Float:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Int:\n",
      " tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n",
      "Bool:\n",
      " tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False, False]])\n",
      "View new shape... tensor([[0., 0., 0., 0., 0., 0.]])\n",
      "Algebraic operations are overloaded:\n",
      " tensor([[-1.7100, -2.5753],\n",
      "        [ 0.4180,  0.2967],\n",
      "        [ 0.1944,  0.6829]]) \n",
      "+\n",
      " tensor([[ 0.5235, -0.2987],\n",
      "        [-1.4283, -0.2475],\n",
      "        [ 0.1088, -0.4696]]) \n",
      "=\n",
      " tensor([[-1.1865, -2.8739],\n",
      "        [-1.0103,  0.0492],\n",
      "        [ 0.3032,  0.2133]])\n"
     ]
    }
   ],
   "source": [
    "# Very similar syntax to numpy.\n",
    "zero_torch = torch.zeros((3, 2))\n",
    "\n",
    "print('zero_torch is of type {:s}'.format(str(type(zero_torch))))\n",
    "\n",
    "# Torch -> Numpy: simply call the numpy() method.\n",
    "zero_np = np.zeros((3, 2))\n",
    "assert (zero_torch.numpy() == zero_np).all()\n",
    "\n",
    "# Numpy -> Torch: simply call the corresponding function on the np.array.\n",
    "zero_torch_float = torch.FloatTensor(zero_np)\n",
    "print('Float:\\n', zero_torch_float)\n",
    "zero_torch_int = torch.LongTensor(zero_np)\n",
    "print('Int:\\n', zero_torch_int)\n",
    "zero_torch_bool = torch.BoolTensor(zero_np)\n",
    "print('Bool:\\n', zero_torch_bool)\n",
    "\n",
    "# Reshape\n",
    "print('View new shape...', zero_torch.view(1, 6))\n",
    "# Note that print(zero_torch.reshape(1, 6)) would work too.\n",
    "# The difference is in how memory is handled (view imposes contiguity).\n",
    "\n",
    "# Algebra\n",
    "a = torch.randn((3, 2))\n",
    "b = torch.randn((3, 2))\n",
    "print('Algebraic operations are overloaded:\\n', a, '\\n+\\n', b, '\\n=\\n', a+b )\n",
    "\n",
    "# More generally, torch shares the syntax of many attributes and functions with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Initial guess: tensor([-0.1295])\n",
      "Final estimate: tensor([2.0118])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor is a similar yet more complicated data structure than np.array.\n",
    "# It is basically a static array of number but may also contain an overlay to \n",
    "# handle automatic differentiation (i.e keeping track of the gradient and which \n",
    "# tensors depend on which).\n",
    "# To access the static array embedded in a tensor, simply call the detach() method\n",
    "print(zero_torch.detach())\n",
    "\n",
    "# When inside a function performing automatic differentiation (basically when training \n",
    "# a neural network), never use detach() otherwise meta information regarding gradients\n",
    "# will be lost, effectively freezing the variable and preventing backprop for it. \n",
    "# However when returning the result of training, do use detach() to save memory \n",
    "# (the naked tensor data uses much less memory than the full-blown tensor with gradient\n",
    "# management, and is much less prone to mistake such as bad copy and memory leak).\n",
    "\n",
    "# We will solve theta * x = y in theta for x=1 and y=2\n",
    "x = torch.ones(1)\n",
    "y = 2 * torch.ones(1)\n",
    "\n",
    "# Actually by default torch does not add the gradient management overlay\n",
    "# when declaring tensors like this. To force it, add requires_grad=True.\n",
    "theta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Optimisation routine\n",
    "# (Adam is a sophisticated variant of SGD, with adaptive step).\n",
    "optimizer = optim.Adam(params=[theta], lr=0.1)\n",
    "\n",
    "# Loss function\n",
    "print('Initial guess:', theta.detach())\n",
    "\n",
    "for _ in range(100):\n",
    "    # By default, torch accumulates gradients in memory.\n",
    "    # To obtain the desired gradient descent beahviour,\n",
    "    # just clean the cached gradients using the following line:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Quadratic loss (* and ** are overloaded so that torch\n",
    "    # knows how to differentiate them)\n",
    "    loss = (y - theta * x) ** 2\n",
    "    \n",
    "    # Apply the chain rule to automatically compute gradients\n",
    "    # for all relevant tensors.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Run one step of optimisation routine.\n",
    "    optimizer.step()\n",
    "    \n",
    "print('Final estimate:', theta.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the environment\n",
    "\n",
    "### 1 - Define the GLOBAL parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "# Capacity of the replay buffer\n",
    "BUFFER_CAPACITY = 16384 # 10000\n",
    "# Update target net every ... episodes\n",
    "UPDATE_TARGET_EVERY = 32 # 20\n",
    "\n",
    "# Initial value of epsilon\n",
    "EPSILON_START = 1.0\n",
    "# Parameter to decrease epsilon\n",
    "DECREASE_EPSILON = 200\n",
    "# Minimum value of epislon\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Number of training episodes\n",
    "N_EPISODES = 200\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# create instance of replay buffer\n",
    "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network and target network\n",
    "hidden_size = 128\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "q_net = Net(obs_size, hidden_size, n_actions)\n",
    "target_net = Net(obs_size, hidden_size, n_actions)\n",
    "\n",
    "# objective and optimizer\n",
    "objective = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 0 (to do at home, not during the live session)\n",
    "\n",
    "With your own word, explain the intuition behind DQN. Recall the main parts of the aformentionned algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(states):\n",
    "    \"\"\"\n",
    "    Compute Q function for a list of states\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        states_v = torch.FloatTensor([states])\n",
    "        output = q_net.forward(states_v).detach().numpy()  # shape (1, len(states), n_actions)\n",
    "    return output[0, :, :]  # shape (len(states), n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "Implement the `eval_dqn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn(n_sim=5):\n",
    "    \"\"\"\n",
    "    ** TO BE IMPLEMENTED **\n",
    "    \n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Implement the `choose_action` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    ** TO BE IMPLEMENTED **\n",
    "    \n",
    "    Return action according to an epsilon-greedy exploration policy\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Implement the `update` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    ** TO BE COMPLETED **\n",
    "    \"\"\"\n",
    "    \n",
    "    # add data to replay buffer\n",
    "    if done:\n",
    "        next_state = None\n",
    "    replay_buffer.push(state, action, reward, next_state)\n",
    "    \n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return np.inf\n",
    "    \n",
    "    # get batch\n",
    "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Compute loss - TO BE IMPLEMENTED!\n",
    "    values  = torch.zeros(BATCH_SIZE)   # to be computed using batch\n",
    "    targets = torch.zeros(BATCH_SIZE)   # to be computed using batch\n",
    "    loss = objective(values, targets)\n",
    "     \n",
    "    # Optimize the model - UNCOMMENT!\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "    return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Train a DQN on the `env` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 5 , reward =  0.0\n",
      "episode = 10 , reward =  0.0\n",
      "episode = 15 , reward =  0.0\n",
      "episode = 20 , reward =  0.0\n",
      "episode = 25 , reward =  0.0\n",
      "episode = 30 , reward =  0.0\n",
      "episode = 35 , reward =  0.0\n",
      "episode = 40 , reward =  0.0\n",
      "episode = 45 , reward =  0.0\n",
      "episode = 50 , reward =  0.0\n",
      "episode = 55 , reward =  0.0\n",
      "episode = 60 , reward =  0.0\n",
      "episode = 65 , reward =  0.0\n",
      "episode = 70 , reward =  0.0\n",
      "episode = 75 , reward =  0.0\n",
      "episode = 80 , reward =  0.0\n",
      "episode = 85 , reward =  0.0\n",
      "episode = 90 , reward =  0.0\n",
      "episode = 95 , reward =  0.0\n",
      "episode = 100 , reward =  0.0\n",
      "episode = 105 , reward =  0.0\n",
      "episode = 110 , reward =  0.0\n",
      "episode = 115 , reward =  0.0\n",
      "episode = 120 , reward =  0.0\n",
      "episode = 125 , reward =  0.0\n",
      "episode = 130 , reward =  0.0\n",
      "episode = 135 , reward =  0.0\n",
      "episode = 140 , reward =  0.0\n",
      "episode = 145 , reward =  0.0\n",
      "episode = 150 , reward =  0.0\n",
      "episode = 155 , reward =  0.0\n",
      "episode = 160 , reward =  0.0\n",
      "episode = 165 , reward =  0.0\n",
      "episode = 170 , reward =  0.0\n",
      "episode = 175 , reward =  0.0\n",
      "episode = 180 , reward =  0.0\n",
      "episode = 185 , reward =  0.0\n",
      "episode = 190 , reward =  0.0\n",
      "episode = 195 , reward =  0.0\n",
      "episode = 200 , reward =  0.0\n",
      "\n",
      "mean reward after training =  0.0\n"
     ]
    }
   ],
   "source": [
    "EVAL_EVERY = 5\n",
    "REWARD_THRESHOLD = 199\n",
    "\n",
    "def train():\n",
    "    state = env.reset()\n",
    "    epsilon = EPSILON_START\n",
    "    ep = 0\n",
    "    total_time = 0\n",
    "    while ep < N_EPISODES:\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        # take action and update replay buffer and networks\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        loss = update(state, action, reward, next_state, done)\n",
    "\n",
    "        # update state\n",
    "        state = next_state\n",
    "\n",
    "        # end episode if done\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            ep   += 1\n",
    "            if ( (ep+1)% EVAL_EVERY == 0):\n",
    "                rewards = eval_dqn()\n",
    "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
    "                    break\n",
    "\n",
    "            # update target network\n",
    "            if ep % UPDATE_TARGET_EVERY == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "            # decrease epsilon\n",
    "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
    "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
    "\n",
    "        total_time += 1\n",
    "\n",
    "# Run the training loop\n",
    "train()\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_dqn(20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Experiment the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video():\n",
    "    html = []\n",
    "    for mp4 in Path(\"videos\").glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    \n",
    "env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
    "\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = choose_action(state, 0.0)\n",
    "        state, reward, done, info = env.step(action)\n",
    "env.close()\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments: Do It Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the set of global parameters:\n",
    "```\n",
    "# Environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "# Capacity of the replay buffer\n",
    "BUFFER_CAPACITY = 16384 # 10000\n",
    "# Update target net every ... episodes\n",
    "UPDATE_TARGET_EVERY = 32 # 20\n",
    "\n",
    "# Initial value of epsilon\n",
    "EPSILON_START = 1.0\n",
    "# Parameter to decrease epsilon\n",
    "DECREASE_EPSILON = 200\n",
    "# Minimum value of epislon\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Number of training episodes\n",
    "N_EPISODES = 200\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "Craft an experiment and study the influence of the `BUFFER_CAPACITY` on the learning process (speed of *convergence*, training curves...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "Craft an experiment and study the influence of the `UPDATE_TARGET_EVERY` on the learning process (speed of *convergence*, training curves...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "\n",
    "If you have the computer power to do so, try to do a grid search on those two hyper-parameters and comment the results. Otherwise, study the influence of another hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: SAIL-DQN\n",
    "\n",
    "\n",
    "`choose_action`, `get_q` and `eval_dqn` remain the same.\n",
    "\n",
    "To be implemented:\n",
    "* `update_sail`, compared to `update`, modifies $y_i$ as explained above.\n",
    "* `train_sail` adds several steps to `train`. \n",
    "\n",
    "Tip #1: `replay_buffer` now contains returns as well.\n",
    "\n",
    "Tip #2: in the computed advantage, use $Q(s_i, a_i, \\theta^-)$, not $Q(s_i, a_i)$. It makes the bonus more stable.\n",
    "\n",
    "Tip #3: `torch.maximum` can be used to compute the element-wise max between two arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "\n",
    "Implement `update_sail` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " def update_sail(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    ** TO BE COMPLETED **\n",
    "    \"\"\"\n",
    "    \n",
    "    # add data to temporary replay buffer\n",
    "    if done:\n",
    "        next_state = None\n",
    "    replay_buffer_temp.push(state, action, reward, next_state)\n",
    "    \n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return np.inf\n",
    "    \n",
    "    # get batch\n",
    "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    # Compute loss - TO BE IMPLEMENTED!\n",
    "    values  = torch.zeros(BATCH_SIZE)   # to be computed using batch\n",
    "    targets = torch.zeros(BATCH_SIZE)   # to be computed using batch\n",
    "    loss = objective(values, targets)\n",
    "     \n",
    "    # Optimize the model - UNCOMMENT!\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "    return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "\n",
    "Implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'replay_buffer_temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c97f49539412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Run the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrain_sail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Evaluate the final policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-c97f49539412>\u001b[0m in \u001b[0;36mtrain_sail\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# take action and update replay buffer and networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_sail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# update state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c5a2e9f1bcc2>\u001b[0m in \u001b[0;36mupdate_sail\u001b[0;34m(state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m        \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m    \u001b[0mreplay_buffer_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'replay_buffer_temp' is not defined"
     ]
    }
   ],
   "source": [
    "def get_episode_returns(rewards):\n",
    "    returns_reversed = accumulate(rewards[::-1],\n",
    "                                lambda x, y: x*GAMMA + y)\n",
    "    return list(returns_reversed)[::-1]\n",
    "\n",
    "def train_sail():\n",
    "    state = env.reset()\n",
    "    epsilon = EPSILON_START\n",
    "    ep = 0\n",
    "    total_time = 0\n",
    "    while ep < N_EPISODES:\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        # take action and update replay buffer and networks\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        loss = update_sail(state, action, reward, next_state, done)\n",
    "\n",
    "        # update state\n",
    "        state = next_state\n",
    "\n",
    "        # end episode if done\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            ep   += 1\n",
    "            if ( (ep+1)% EVAL_EVERY == 0):\n",
    "                rewards = eval_dqn()\n",
    "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
    "                    break\n",
    "\n",
    "            # fetch transitions from the temporary memory\n",
    "            transitions = replay_buffer_temp.memory\n",
    "\n",
    "            # calculate episode returns\n",
    "            # TO IMPLEMENT\n",
    "\n",
    "            # transfer transitions completed with returns to main memory\n",
    "            # TO IMPLEMENT\n",
    "\n",
    "            # reset the temporary memory\n",
    "            # TO IMPLEMENT\n",
    "\n",
    "            # update target network\n",
    "            if ep % UPDATE_TARGET_EVERY == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "            # decrease epsilon\n",
    "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
    "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
    "\n",
    "        total_time += 1\n",
    "\n",
    "# Run the training loop\n",
    "train_sail()\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_dqn(20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11\n",
    "\n",
    "Display your policy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import base64\n",
    "\n",
    "def show_video():\n",
    "    html = []\n",
    "    for mp4 in Path(\"videos\").glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    \n",
    "env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
    "\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = choose_action(state, 0.0)\n",
    "        state, reward, done, info = env.step(action)\n",
    "env.close()\n",
    "# show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td4-env",
   "language": "python",
   "name": "td4-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
