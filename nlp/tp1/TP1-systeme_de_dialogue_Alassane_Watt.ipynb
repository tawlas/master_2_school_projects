{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auteur: Alassane Watt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import multiprocessing\n",
    "import unicodedata\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(\"[^A-Za-z0-9']+\", ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('fr_core_news_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "# def cleaning(doc):\n",
    "#     # Lemmatizes and removes stopwords\n",
    "#     # doc needs to be a spacy Doc object\n",
    "#     txt = [str(token) for token in doc if not token.is_stop]\n",
    "#     # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "#     # if a sentence is only one or two words long,\n",
    "#     # the benefit for the training is very small\n",
    "#     if len(txt) > 2:\n",
    "#         return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(document):\n",
    "    # strip accents\n",
    "    document = strip_accents(document)\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "#         tokens = document.split()\n",
    "#         tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "#         tokens = [word for word in tokens if word not in en_stop]\n",
    "#         tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "#         preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = 100\n",
    "wv_min_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus QUAERO_FrenchMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_path = \"./QUAERO_FrenchMed/QUAERO_FrenchMed_traindev.ospl\"\n",
    "with open(med_path) as f:\n",
    "    sentences = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   EMEA / H / C / 551\n",
       "1                                               PRIALT\n",
       "2                             Qu ’ est ce que Prialt ?\n",
       "3    Prialt est une solution pour perfusion contena...\n",
       "4              Dans quel cas Prialt est - il utilisé ?\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.Series(sentences)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           emea c 551\n",
       "1                                               prialt\n",
       "2                                qu est ce que prialt \n",
       "3    prialt est une solution pour perfusion contena...\n",
       "4                 dans quel cas prialt est il utilise \n",
       "5    prialt est indique pour le traitement des doul...\n",
       "6    comme le nombre de patients souffrant de doule...\n",
       "7                       comment prialt est il utilise \n",
       "8    le traitement par prialt ne doit etre realise ...\n",
       "9    prialt doit etre administre en perfusion conti...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_med = [s.split(\" \") for s in df.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:56:34: collecting all words and their counts\n",
      "INFO - 18:56:34: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:56:34: collected 7826 word types from a corpus of 42967 raw words and 3091 sentences\n",
      "INFO - 18:56:34: Loading a fresh vocabulary\n",
      "INFO - 18:56:34: effective_min_count=1 retains 7826 unique words (100% of original 7826, drops 0)\n",
      "INFO - 18:56:34: effective_min_count=1 leaves 42967 word corpus (100% of original 42967, drops 0)\n",
      "INFO - 18:56:34: deleting the raw counts dictionary of 7826 items\n",
      "INFO - 18:56:34: sample=0.001 downsamples 34 most-common words\n",
      "INFO - 18:56:34: downsampling leaves estimated 31810 word corpus (74.0% of prior 42967)\n",
      "INFO - 18:56:34: constructing a huffman tree from 7826 words\n",
      "INFO - 18:56:34: built huffman tree with maximum node depth 16\n",
      "INFO - 18:56:34: estimated required memory for 7826 words and 100 dimensions: 14869400 bytes\n",
      "INFO - 18:56:34: resetting layer weights\n",
      "INFO - 18:56:34: training model with 3 workers on 7826 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:35: EPOCH - 1 : training on 42967 raw words (31767 effective words) took 0.1s, 212741 effective words/s\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:35: EPOCH - 2 : training on 42967 raw words (31878 effective words) took 0.1s, 219309 effective words/s\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:35: EPOCH - 3 : training on 42967 raw words (31765 effective words) took 0.1s, 216595 effective words/s\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:35: EPOCH - 4 : training on 42967 raw words (31762 effective words) took 0.2s, 203755 effective words/s\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:35: EPOCH - 5 : training on 42967 raw words (31842 effective words) took 0.2s, 202317 effective words/s\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:35: EPOCH - 6 : training on 42967 raw words (31810 effective words) took 0.2s, 211264 effective words/s\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:36: EPOCH - 7 : training on 42967 raw words (31728 effective words) took 0.2s, 211103 effective words/s\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:36: EPOCH - 8 : training on 42967 raw words (31816 effective words) took 0.2s, 210763 effective words/s\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:36: EPOCH - 9 : training on 42967 raw words (31796 effective words) took 0.2s, 202516 effective words/s\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:36: EPOCH - 10 : training on 42967 raw words (31813 effective words) took 0.2s, 211712 effective words/s\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:36: EPOCH - 11 : training on 42967 raw words (31831 effective words) took 0.2s, 208918 effective words/s\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:36: EPOCH - 12 : training on 42967 raw words (31810 effective words) took 0.2s, 211033 effective words/s\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:37: EPOCH - 13 : training on 42967 raw words (31716 effective words) took 0.2s, 202436 effective words/s\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:37: EPOCH - 14 : training on 42967 raw words (31838 effective words) took 0.2s, 205922 effective words/s\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:37: EPOCH - 15 : training on 42967 raw words (31764 effective words) took 0.2s, 209271 effective words/s\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:37: EPOCH - 16 : training on 42967 raw words (31782 effective words) took 0.2s, 211535 effective words/s\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:37: EPOCH - 17 : training on 42967 raw words (31800 effective words) took 0.2s, 204516 effective words/s\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:37: EPOCH - 18 : training on 42967 raw words (31812 effective words) took 0.1s, 215243 effective words/s\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:37: EPOCH - 19 : training on 42967 raw words (31771 effective words) took 0.2s, 194574 effective words/s\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:38: EPOCH - 20 : training on 42967 raw words (31829 effective words) took 0.2s, 209663 effective words/s\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:38: EPOCH - 21 : training on 42967 raw words (31784 effective words) took 0.2s, 202509 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:56:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:38: EPOCH - 22 : training on 42967 raw words (31802 effective words) took 0.2s, 204973 effective words/s\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:38: EPOCH - 23 : training on 42967 raw words (31904 effective words) took 0.2s, 199735 effective words/s\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:38: EPOCH - 24 : training on 42967 raw words (31920 effective words) took 0.2s, 197791 effective words/s\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:38: EPOCH - 25 : training on 42967 raw words (31752 effective words) took 0.1s, 213897 effective words/s\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:39: EPOCH - 26 : training on 42967 raw words (31832 effective words) took 0.1s, 223042 effective words/s\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:39: EPOCH - 27 : training on 42967 raw words (31814 effective words) took 0.1s, 221197 effective words/s\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:39: EPOCH - 28 : training on 42967 raw words (31909 effective words) took 0.2s, 212693 effective words/s\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:39: EPOCH - 29 : training on 42967 raw words (31813 effective words) took 0.2s, 208363 effective words/s\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:39: EPOCH - 30 : training on 42967 raw words (31846 effective words) took 0.2s, 209381 effective words/s\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:39: EPOCH - 31 : training on 42967 raw words (31825 effective words) took 0.2s, 210158 effective words/s\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:40: EPOCH - 32 : training on 42967 raw words (31782 effective words) took 0.1s, 212212 effective words/s\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:40: EPOCH - 33 : training on 42967 raw words (31763 effective words) took 0.2s, 203452 effective words/s\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:40: EPOCH - 34 : training on 42967 raw words (31895 effective words) took 0.1s, 219631 effective words/s\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:40: EPOCH - 35 : training on 42967 raw words (31859 effective words) took 0.1s, 221614 effective words/s\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:40: EPOCH - 36 : training on 42967 raw words (31880 effective words) took 0.2s, 210487 effective words/s\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:40: EPOCH - 37 : training on 42967 raw words (31791 effective words) took 0.1s, 214748 effective words/s\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:40: EPOCH - 38 : training on 42967 raw words (31878 effective words) took 0.1s, 218133 effective words/s\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:41: EPOCH - 39 : training on 42967 raw words (31863 effective words) took 0.2s, 211060 effective words/s\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:41: EPOCH - 40 : training on 42967 raw words (31862 effective words) took 0.1s, 219524 effective words/s\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:41: EPOCH - 41 : training on 42967 raw words (31774 effective words) took 0.1s, 216957 effective words/s\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:41: EPOCH - 42 : training on 42967 raw words (31796 effective words) took 0.1s, 218428 effective words/s\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:41: EPOCH - 43 : training on 42967 raw words (31756 effective words) took 0.1s, 221841 effective words/s\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:41: EPOCH - 44 : training on 42967 raw words (31824 effective words) took 0.1s, 223631 effective words/s\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:42: EPOCH - 45 : training on 42967 raw words (31860 effective words) took 0.1s, 220287 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:56:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:42: EPOCH - 46 : training on 42967 raw words (31883 effective words) took 0.1s, 222363 effective words/s\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:42: EPOCH - 47 : training on 42967 raw words (31899 effective words) took 0.1s, 222035 effective words/s\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:42: EPOCH - 48 : training on 42967 raw words (31858 effective words) took 0.1s, 223265 effective words/s\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:42: EPOCH - 49 : training on 42967 raw words (31820 effective words) took 0.1s, 220787 effective words/s\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:42: EPOCH - 50 : training on 42967 raw words (31779 effective words) took 0.1s, 221486 effective words/s\n",
      "INFO - 18:56:42: training on a 2148350 raw words (1590953 effective words) took 7.9s, 202435 effective words/s\n",
      "INFO - 18:56:42: saving Word2Vec object under WE_models/w2v_sg_100D, separately None\n",
      "INFO - 18:56:42: not storing attribute vectors_norm\n",
      "INFO - 18:56:42: not storing attribute cum_table\n",
      "INFO - 18:56:42: saved WE_models/w2v_sg_100D\n"
     ]
    }
   ],
   "source": [
    "sg_model = Word2Vec(sentences=sentences_med, size=wv_dim, sg=1, hs=1, min_count=1, iter=50)\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "sg_model.save('WE_models/w2v_sg_100D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:56:59: collecting all words and their counts\n",
      "INFO - 18:56:59: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:56:59: collected 7826 word types from a corpus of 42967 raw words and 3091 sentences\n",
      "INFO - 18:56:59: Loading a fresh vocabulary\n",
      "INFO - 18:56:59: effective_min_count=1 retains 7826 unique words (100% of original 7826, drops 0)\n",
      "INFO - 18:56:59: effective_min_count=1 leaves 42967 word corpus (100% of original 42967, drops 0)\n",
      "INFO - 18:56:59: deleting the raw counts dictionary of 7826 items\n",
      "INFO - 18:56:59: sample=0.001 downsamples 34 most-common words\n",
      "INFO - 18:56:59: downsampling leaves estimated 31810 word corpus (74.0% of prior 42967)\n",
      "INFO - 18:56:59: constructing a huffman tree from 7826 words\n",
      "INFO - 18:56:59: built huffman tree with maximum node depth 16\n",
      "INFO - 18:56:59: estimated required memory for 7826 words and 100 dimensions: 14869400 bytes\n",
      "INFO - 18:56:59: resetting layer weights\n",
      "INFO - 18:56:59: training model with 3 workers on 7826 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:59: EPOCH - 1 : training on 42967 raw words (31708 effective words) took 0.0s, 642255 effective words/s\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:59: EPOCH - 2 : training on 42967 raw words (31793 effective words) took 0.1s, 613909 effective words/s\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:59: EPOCH - 3 : training on 42967 raw words (31848 effective words) took 0.1s, 621364 effective words/s\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:59: EPOCH - 4 : training on 42967 raw words (31773 effective words) took 0.1s, 568133 effective words/s\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:56:59: EPOCH - 5 : training on 42967 raw words (31810 effective words) took 0.0s, 644807 effective words/s\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:56:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 6 : training on 42967 raw words (31767 effective words) took 0.1s, 611799 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 7 : training on 42967 raw words (31770 effective words) took 0.1s, 556471 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 8 : training on 42967 raw words (31847 effective words) took 0.1s, 461145 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 9 : training on 42967 raw words (31707 effective words) took 0.1s, 561181 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 10 : training on 42967 raw words (31711 effective words) took 0.1s, 609972 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 11 : training on 42967 raw words (31778 effective words) took 0.1s, 587917 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 12 : training on 42967 raw words (31784 effective words) took 0.1s, 594976 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 13 : training on 42967 raw words (31755 effective words) took 0.1s, 576885 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 14 : training on 42967 raw words (31766 effective words) took 0.1s, 612800 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 15 : training on 42967 raw words (31751 effective words) took 0.1s, 577567 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 16 : training on 42967 raw words (31791 effective words) took 0.1s, 529481 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 17 : training on 42967 raw words (31726 effective words) took 0.1s, 567720 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 18 : training on 42967 raw words (31898 effective words) took 0.1s, 551624 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 19 : training on 42967 raw words (31742 effective words) took 0.1s, 626324 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 20 : training on 42967 raw words (31833 effective words) took 0.1s, 628536 effective words/s\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:00: EPOCH - 21 : training on 42967 raw words (31837 effective words) took 0.1s, 621368 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:57:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 22 : training on 42967 raw words (31819 effective words) took 0.1s, 579225 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 23 : training on 42967 raw words (31846 effective words) took 0.1s, 580400 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 24 : training on 42967 raw words (31790 effective words) took 0.1s, 578874 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 25 : training on 42967 raw words (31774 effective words) took 0.1s, 585424 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 26 : training on 42967 raw words (31770 effective words) took 0.0s, 652310 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 27 : training on 42967 raw words (31743 effective words) took 0.0s, 636448 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 28 : training on 42967 raw words (31775 effective words) took 0.0s, 651210 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 29 : training on 42967 raw words (31783 effective words) took 0.0s, 638310 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 30 : training on 42967 raw words (31801 effective words) took 0.0s, 658159 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 31 : training on 42967 raw words (31783 effective words) took 0.0s, 674336 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 32 : training on 42967 raw words (31828 effective words) took 0.0s, 676774 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 33 : training on 42967 raw words (31748 effective words) took 0.0s, 647773 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 34 : training on 42967 raw words (31854 effective words) took 0.0s, 682294 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 35 : training on 42967 raw words (31805 effective words) took 0.0s, 661813 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 36 : training on 42967 raw words (31829 effective words) took 0.0s, 643227 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 37 : training on 42967 raw words (31831 effective words) took 0.0s, 656039 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 38 : training on 42967 raw words (31864 effective words) took 0.0s, 641685 effective words/s\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:01: EPOCH - 39 : training on 42967 raw words (31899 effective words) took 0.1s, 633664 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 40 : training on 42967 raw words (31741 effective words) took 0.1s, 633874 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 41 : training on 42967 raw words (31815 effective words) took 0.0s, 644230 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 42 : training on 42967 raw words (31821 effective words) took 0.0s, 686170 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 43 : training on 42967 raw words (31762 effective words) took 0.0s, 673176 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 44 : training on 42967 raw words (31767 effective words) took 0.0s, 650459 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 45 : training on 42967 raw words (31886 effective words) took 0.1s, 620458 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 46 : training on 42967 raw words (31787 effective words) took 0.0s, 663435 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 47 : training on 42967 raw words (31884 effective words) took 0.1s, 615848 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 48 : training on 42967 raw words (31909 effective words) took 0.0s, 653679 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 49 : training on 42967 raw words (31825 effective words) took 0.0s, 663116 effective words/s\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:02: EPOCH - 50 : training on 42967 raw words (31801 effective words) took 0.1s, 635253 effective words/s\n",
      "INFO - 18:57:02: training on a 2148350 raw words (1589935 effective words) took 3.0s, 537257 effective words/s\n",
      "INFO - 18:57:02: saving Word2Vec object under WE_models/w2v_cbow_100D, separately None\n",
      "INFO - 18:57:02: not storing attribute vectors_norm\n",
      "INFO - 18:57:02: not storing attribute cum_table\n",
      "INFO - 18:57:02: saved WE_models/w2v_cbow_100D\n"
     ]
    }
   ],
   "source": [
    "cbow_model = Word2Vec(sentences=sentences_med, size=wv_dim, sg=0, hs=1, min_count=1, iter=50)\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "cbow_model.save('WE_models/w2v_cbow_100D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fatsttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:17:43: resetting layer weights\n",
      "INFO - 19:17:54: collecting all words and their counts\n",
      "INFO - 19:17:54: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 19:17:54: collected 7826 word types from a corpus of 42967 raw words and 3091 sentences\n",
      "INFO - 19:17:54: Loading a fresh vocabulary\n",
      "INFO - 19:17:54: effective_min_count=1 retains 7826 unique words (100% of original 7826, drops 0)\n",
      "INFO - 19:17:54: effective_min_count=1 leaves 42967 word corpus (100% of original 42967, drops 0)\n",
      "INFO - 19:17:55: deleting the raw counts dictionary of 7826 items\n",
      "INFO - 19:17:55: sample=0.001 downsamples 34 most-common words\n",
      "INFO - 19:17:55: downsampling leaves estimated 31810 word corpus (74.0% of prior 42967)\n",
      "INFO - 19:17:55: constructing a huffman tree from 7826 words\n",
      "INFO - 19:17:55: built huffman tree with maximum node depth 16\n",
      "INFO - 19:17:55: estimated required memory for 7826 words, 61331 buckets and 100 dimensions: 41558608 bytes\n",
      "INFO - 19:17:55: resetting layer weights\n",
      "INFO - 19:18:01: training model with 3 workers on 7826 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:01: EPOCH - 1 : training on 42967 raw words (31727 effective words) took 0.3s, 90891 effective words/s\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:01: EPOCH - 2 : training on 42967 raw words (31769 effective words) took 0.2s, 137007 effective words/s\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:01: EPOCH - 3 : training on 42967 raw words (31807 effective words) took 0.2s, 136632 effective words/s\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:02: EPOCH - 4 : training on 42967 raw words (31772 effective words) took 0.2s, 133370 effective words/s\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:02: EPOCH - 5 : training on 42967 raw words (31831 effective words) took 0.2s, 129822 effective words/s\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:02: EPOCH - 6 : training on 42967 raw words (31674 effective words) took 0.2s, 129110 effective words/s\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:02: EPOCH - 7 : training on 42967 raw words (31845 effective words) took 0.2s, 135430 effective words/s\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:03: EPOCH - 8 : training on 42967 raw words (31849 effective words) took 0.2s, 138421 effective words/s\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:03: EPOCH - 9 : training on 42967 raw words (31813 effective words) took 0.2s, 138138 effective words/s\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:03: EPOCH - 10 : training on 42967 raw words (31805 effective words) took 0.2s, 132658 effective words/s\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:03: EPOCH - 11 : training on 42967 raw words (31865 effective words) took 0.2s, 136321 effective words/s\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:04: EPOCH - 12 : training on 42967 raw words (31808 effective words) took 0.2s, 134836 effective words/s\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:04: EPOCH - 13 : training on 42967 raw words (31791 effective words) took 0.2s, 136734 effective words/s\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:04: EPOCH - 14 : training on 42967 raw words (31864 effective words) took 0.2s, 134841 effective words/s\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:04: EPOCH - 15 : training on 42967 raw words (31818 effective words) took 0.2s, 137052 effective words/s\n",
      "INFO - 19:18:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:05: EPOCH - 16 : training on 42967 raw words (31727 effective words) took 0.2s, 134297 effective words/s\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:05: EPOCH - 17 : training on 42967 raw words (31755 effective words) took 0.2s, 129058 effective words/s\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:05: EPOCH - 18 : training on 42967 raw words (31783 effective words) took 0.2s, 139292 effective words/s\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:05: EPOCH - 19 : training on 42967 raw words (31861 effective words) took 0.2s, 139128 effective words/s\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:06: EPOCH - 20 : training on 42967 raw words (31770 effective words) took 0.2s, 135107 effective words/s\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:18:06: EPOCH - 21 : training on 42967 raw words (31819 effective words) took 0.2s, 136423 effective words/s\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:06: EPOCH - 22 : training on 42967 raw words (31735 effective words) took 0.2s, 135084 effective words/s\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:06: EPOCH - 23 : training on 42967 raw words (31822 effective words) took 0.3s, 122745 effective words/s\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:07: EPOCH - 24 : training on 42967 raw words (31763 effective words) took 0.2s, 129502 effective words/s\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:07: EPOCH - 25 : training on 42967 raw words (31821 effective words) took 0.2s, 132781 effective words/s\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:07: EPOCH - 26 : training on 42967 raw words (31762 effective words) took 0.2s, 133879 effective words/s\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:07: EPOCH - 27 : training on 42967 raw words (31841 effective words) took 0.2s, 133053 effective words/s\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:08: EPOCH - 28 : training on 42967 raw words (31835 effective words) took 0.3s, 126212 effective words/s\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:08: EPOCH - 29 : training on 42967 raw words (31800 effective words) took 0.3s, 123069 effective words/s\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:08: EPOCH - 30 : training on 42967 raw words (31797 effective words) took 0.2s, 130082 effective words/s\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:08: EPOCH - 31 : training on 42967 raw words (31801 effective words) took 0.2s, 128049 effective words/s\n",
      "INFO - 19:18:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:09: EPOCH - 32 : training on 42967 raw words (31711 effective words) took 0.3s, 126723 effective words/s\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:09: EPOCH - 33 : training on 42967 raw words (31744 effective words) took 0.2s, 128216 effective words/s\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:09: EPOCH - 34 : training on 42967 raw words (31861 effective words) took 0.2s, 130884 effective words/s\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:09: EPOCH - 35 : training on 42967 raw words (31809 effective words) took 0.2s, 129560 effective words/s\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:10: EPOCH - 36 : training on 42967 raw words (31838 effective words) took 0.2s, 132047 effective words/s\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:10: EPOCH - 37 : training on 42967 raw words (31848 effective words) took 0.3s, 124832 effective words/s\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:10: EPOCH - 38 : training on 42967 raw words (31971 effective words) took 0.2s, 130955 effective words/s\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:10: EPOCH - 39 : training on 42967 raw words (31817 effective words) took 0.2s, 134435 effective words/s\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:11: EPOCH - 40 : training on 42967 raw words (31941 effective words) took 0.2s, 136570 effective words/s\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:11: EPOCH - 41 : training on 42967 raw words (31929 effective words) took 0.2s, 132959 effective words/s\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:11: EPOCH - 42 : training on 42967 raw words (31760 effective words) took 0.2s, 136512 effective words/s\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:11: EPOCH - 43 : training on 42967 raw words (31842 effective words) took 0.2s, 134512 effective words/s\n",
      "INFO - 19:18:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:12: EPOCH - 44 : training on 42967 raw words (31782 effective words) took 0.2s, 134592 effective words/s\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:18:12: EPOCH - 45 : training on 42967 raw words (31788 effective words) took 0.2s, 135442 effective words/s\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:12: EPOCH - 46 : training on 42967 raw words (31815 effective words) took 0.2s, 133008 effective words/s\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:12: EPOCH - 47 : training on 42967 raw words (31847 effective words) took 0.2s, 133291 effective words/s\n",
      "INFO - 19:18:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:13: EPOCH - 48 : training on 42967 raw words (31820 effective words) took 0.2s, 135407 effective words/s\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:13: EPOCH - 49 : training on 42967 raw words (31770 effective words) took 0.2s, 134301 effective words/s\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:18:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:18:13: EPOCH - 50 : training on 42967 raw words (31732 effective words) took 0.2s, 136911 effective words/s\n",
      "INFO - 19:18:13: training on a 2148350 raw words (1590355 effective words) took 12.5s, 127190 effective words/s\n",
      "INFO - 19:18:14: saving FastText object under WE_models/w2v_ft_100D, separately None\n",
      "INFO - 19:18:14: storing np array 'vectors_ngrams' to WE_models/w2v_ft_100D.wv.vectors_ngrams.npy\n",
      "INFO - 19:18:19: not storing attribute vectors_norm\n",
      "INFO - 19:18:19: not storing attribute vectors_vocab_norm\n",
      "INFO - 19:18:19: not storing attribute vectors_ngrams_norm\n",
      "INFO - 19:18:19: not storing attribute buckets_word\n",
      "INFO - 19:18:19: storing np array 'vectors_ngrams_lockf' to WE_models/w2v_ft_100D.trainables.vectors_ngrams_lockf.npy\n",
      "INFO - 19:18:25: saved WE_models/w2v_ft_100D\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = gensim.models.fasttext.FastText(sentences=sentences_med, size=wv_dim, sg=0, hs=1, min_count=1, iter=50)\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "fasttext_model.save('WE_models/w2v_ft_100D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus QUAERO_FrenchPress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "press_path = \"./QUAERO_FrenchPress/QUAERO_FrenchPress_traindev.ospl\"\n",
    "with open(press_path) as f:\n",
    "    sentences = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.Series(sentences)\n",
    "sentences = sentences.apply(preprocess_text)\n",
    "sentences = sentences.to_list()\n",
    "sentences_press = [s.split(\" \") for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:57:31: collecting all words and their counts\n",
      "INFO - 18:57:31: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:57:31: PROGRESS: at sentence #10000, processed 278773 words, keeping 18477 word types\n",
      "INFO - 18:57:31: PROGRESS: at sentence #20000, processed 531101 words, keeping 26360 word types\n",
      "INFO - 18:57:31: PROGRESS: at sentence #30000, processed 809820 words, keeping 31793 word types\n",
      "INFO - 18:57:31: collected 35706 word types from a corpus of 1091998 raw words and 38548 sentences\n",
      "INFO - 18:57:31: Loading a fresh vocabulary\n",
      "INFO - 18:57:31: effective_min_count=1 retains 35706 unique words (100% of original 35706, drops 0)\n",
      "INFO - 18:57:31: effective_min_count=1 leaves 1091998 word corpus (100% of original 1091998, drops 0)\n",
      "INFO - 18:57:31: deleting the raw counts dictionary of 35706 items\n",
      "INFO - 18:57:31: sample=0.001 downsamples 42 most-common words\n",
      "INFO - 18:57:31: downsampling leaves estimated 807950 word corpus (74.0% of prior 1091998)\n",
      "INFO - 18:57:31: constructing a huffman tree from 35706 words\n",
      "INFO - 18:57:34: built huffman tree with maximum node depth 20\n",
      "INFO - 18:57:34: estimated required memory for 35706 words and 100 dimensions: 67841400 bytes\n",
      "INFO - 18:57:34: resetting layer weights\n",
      "INFO - 18:57:35: training model with 3 workers on 35706 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO - 18:57:36: EPOCH 1 - PROGRESS: at 28.43% examples, 220848 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:37: EPOCH 1 - PROGRESS: at 60.03% examples, 221202 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:57:38: EPOCH 1 - PROGRESS: at 85.83% examples, 223432 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:38: EPOCH - 1 : training on 1091998 raw words (807507 effective words) took 3.6s, 223143 effective words/s\n",
      "INFO - 18:57:39: EPOCH 2 - PROGRESS: at 28.43% examples, 219418 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:57:40: EPOCH 2 - PROGRESS: at 60.03% examples, 218935 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:41: EPOCH 2 - PROGRESS: at 84.96% examples, 219691 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:42: EPOCH - 2 : training on 1091998 raw words (807688 effective words) took 3.7s, 219174 effective words/s\n",
      "INFO - 18:57:43: EPOCH 3 - PROGRESS: at 22.33% examples, 181632 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:44: EPOCH 3 - PROGRESS: at 50.33% examples, 187598 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:45: EPOCH 3 - PROGRESS: at 76.26% examples, 188719 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:46: EPOCH 3 - PROGRESS: at 97.24% examples, 188052 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 18:57:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:46: EPOCH - 3 : training on 1091998 raw words (808161 effective words) took 4.3s, 189144 effective words/s\n",
      "INFO - 18:57:47: EPOCH 4 - PROGRESS: at 22.33% examples, 178548 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:48: EPOCH 4 - PROGRESS: at 53.83% examples, 192954 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:49: EPOCH 4 - PROGRESS: at 80.02% examples, 196902 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:50: EPOCH - 4 : training on 1091998 raw words (807765 effective words) took 4.0s, 199554 effective words/s\n",
      "INFO - 18:57:51: EPOCH 5 - PROGRESS: at 24.75% examples, 198573 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:52: EPOCH 5 - PROGRESS: at 53.83% examples, 196753 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:53: EPOCH 5 - PROGRESS: at 78.33% examples, 197548 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:54: EPOCH - 5 : training on 1091998 raw words (807878 effective words) took 4.0s, 199684 effective words/s\n",
      "INFO - 18:57:55: EPOCH 6 - PROGRESS: at 25.99% examples, 196691 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:57:56: EPOCH 6 - PROGRESS: at 57.10% examples, 205056 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:58: EPOCH 6 - PROGRESS: at 82.55% examples, 205502 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:57:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:57:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:57:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:57:58: EPOCH - 6 : training on 1091998 raw words (808078 effective words) took 3.9s, 207292 effective words/s\n",
      "INFO - 18:57:59: EPOCH 7 - PROGRESS: at 25.99% examples, 193697 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:00: EPOCH 7 - PROGRESS: at 55.04% examples, 198325 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:01: EPOCH 7 - PROGRESS: at 78.99% examples, 198099 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:02: EPOCH - 7 : training on 1091998 raw words (808062 effective words) took 4.0s, 200779 effective words/s\n",
      "INFO - 18:58:03: EPOCH 8 - PROGRESS: at 22.33% examples, 181042 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:04: EPOCH 8 - PROGRESS: at 50.33% examples, 186572 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:05: EPOCH 8 - PROGRESS: at 76.26% examples, 187988 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:06: EPOCH 8 - PROGRESS: at 97.24% examples, 189190 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 18:58:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:07: EPOCH - 8 : training on 1091998 raw words (807719 effective words) took 4.2s, 190433 effective words/s\n",
      "INFO - 18:58:08: EPOCH 9 - PROGRESS: at 23.65% examples, 188772 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:09: EPOCH 9 - PROGRESS: at 51.57% examples, 193566 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:10: EPOCH 9 - PROGRESS: at 76.26% examples, 192147 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:11: EPOCH 9 - PROGRESS: at 96.24% examples, 191724 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:11: EPOCH - 9 : training on 1091998 raw words (807584 effective words) took 4.2s, 191145 effective words/s\n",
      "INFO - 18:58:12: EPOCH 10 - PROGRESS: at 22.33% examples, 172823 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:13: EPOCH 10 - PROGRESS: at 50.33% examples, 180794 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:14: EPOCH 10 - PROGRESS: at 76.26% examples, 184148 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:15: EPOCH 10 - PROGRESS: at 96.24% examples, 185304 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:15: EPOCH - 10 : training on 1091998 raw words (808435 effective words) took 4.4s, 185581 effective words/s\n",
      "INFO - 18:58:16: EPOCH 11 - PROGRESS: at 22.33% examples, 180361 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:58:17: EPOCH 11 - PROGRESS: at 50.33% examples, 183924 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:18: EPOCH 11 - PROGRESS: at 76.26% examples, 185306 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:19: EPOCH 11 - PROGRESS: at 96.24% examples, 186755 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:20: EPOCH - 11 : training on 1091998 raw words (807904 effective words) took 4.3s, 187170 effective words/s\n",
      "INFO - 18:58:21: EPOCH 12 - PROGRESS: at 22.33% examples, 182630 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:22: EPOCH 12 - PROGRESS: at 53.83% examples, 194362 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:23: EPOCH 12 - PROGRESS: at 78.99% examples, 198038 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:24: EPOCH - 12 : training on 1091998 raw words (808552 effective words) took 4.1s, 197895 effective words/s\n",
      "INFO - 18:58:25: EPOCH 13 - PROGRESS: at 24.75% examples, 199025 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:26: EPOCH 13 - PROGRESS: at 53.83% examples, 198399 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:27: EPOCH 13 - PROGRESS: at 78.99% examples, 200792 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:28: EPOCH - 13 : training on 1091998 raw words (807875 effective words) took 4.0s, 200132 effective words/s\n",
      "INFO - 18:58:29: EPOCH 14 - PROGRESS: at 24.75% examples, 197734 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:30: EPOCH 14 - PROGRESS: at 53.83% examples, 196241 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:31: EPOCH 14 - PROGRESS: at 78.33% examples, 196175 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:32: EPOCH 14 - PROGRESS: at 100.00% examples, 197190 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 18:58:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:32: EPOCH - 14 : training on 1091998 raw words (808585 effective words) took 4.1s, 197132 effective words/s\n",
      "INFO - 18:58:33: EPOCH 15 - PROGRESS: at 23.65% examples, 188355 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:34: EPOCH 15 - PROGRESS: at 52.66% examples, 195876 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:35: EPOCH 15 - PROGRESS: at 77.68% examples, 195178 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:36: EPOCH 15 - PROGRESS: at 98.94% examples, 196026 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 18:58:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:36: EPOCH - 15 : training on 1091998 raw words (807859 effective words) took 4.1s, 196535 effective words/s\n",
      "INFO - 18:58:37: EPOCH 16 - PROGRESS: at 25.99% examples, 191776 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:38: EPOCH 16 - PROGRESS: at 55.84% examples, 201078 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:39: EPOCH 16 - PROGRESS: at 78.99% examples, 198189 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:40: EPOCH - 16 : training on 1091998 raw words (807912 effective words) took 4.1s, 197644 effective words/s\n",
      "INFO - 18:58:41: EPOCH 17 - PROGRESS: at 23.65% examples, 186941 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:42: EPOCH 17 - PROGRESS: at 51.57% examples, 192182 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:43: EPOCH 17 - PROGRESS: at 77.06% examples, 193372 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:44: EPOCH 17 - PROGRESS: at 98.09% examples, 193753 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 18:58:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:44: EPOCH - 17 : training on 1091998 raw words (807731 effective words) took 4.2s, 194514 effective words/s\n",
      "INFO - 18:58:45: EPOCH 18 - PROGRESS: at 22.33% examples, 183423 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:46: EPOCH 18 - PROGRESS: at 51.57% examples, 193427 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:47: EPOCH 18 - PROGRESS: at 77.06% examples, 195487 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:48: EPOCH 18 - PROGRESS: at 98.09% examples, 195868 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 18:58:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:48: EPOCH - 18 : training on 1091998 raw words (807985 effective words) took 4.1s, 196318 effective words/s\n",
      "INFO - 18:58:49: EPOCH 19 - PROGRESS: at 23.65% examples, 191106 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:50: EPOCH 19 - PROGRESS: at 53.83% examples, 197887 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:51: EPOCH 19 - PROGRESS: at 78.33% examples, 197461 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:52: EPOCH 19 - PROGRESS: at 99.16% examples, 196586 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 18:58:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:52: EPOCH - 19 : training on 1091998 raw words (807640 effective words) took 4.1s, 197985 effective words/s\n",
      "INFO - 18:58:53: EPOCH 20 - PROGRESS: at 22.33% examples, 184113 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:54: EPOCH 20 - PROGRESS: at 51.57% examples, 193613 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:55: EPOCH 20 - PROGRESS: at 77.06% examples, 195573 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:58:56: EPOCH 20 - PROGRESS: at 97.24% examples, 193167 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 18:58:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:58:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:58:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:58:57: EPOCH - 20 : training on 1091998 raw words (807845 effective words) took 4.2s, 194204 effective words/s\n",
      "INFO - 18:58:58: EPOCH 21 - PROGRESS: at 22.33% examples, 178425 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:58:59: EPOCH 21 - PROGRESS: at 50.33% examples, 187872 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:00: EPOCH 21 - PROGRESS: at 76.26% examples, 191074 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:01: EPOCH 21 - PROGRESS: at 98.09% examples, 194228 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 18:59:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:01: EPOCH - 21 : training on 1091998 raw words (807741 effective words) took 4.1s, 194748 effective words/s\n",
      "INFO - 18:59:02: EPOCH 22 - PROGRESS: at 25.99% examples, 192023 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:03: EPOCH 22 - PROGRESS: at 55.84% examples, 202072 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:04: EPOCH 22 - PROGRESS: at 79.50% examples, 199396 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:05: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:59:05: EPOCH - 22 : training on 1091998 raw words (808066 effective words) took 4.1s, 197997 effective words/s\n",
      "INFO - 18:59:06: EPOCH 23 - PROGRESS: at 22.33% examples, 183003 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:07: EPOCH 23 - PROGRESS: at 51.57% examples, 193687 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:08: EPOCH 23 - PROGRESS: at 76.26% examples, 193058 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:09: EPOCH 23 - PROGRESS: at 97.24% examples, 192704 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 18:59:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:09: EPOCH - 23 : training on 1091998 raw words (807943 effective words) took 4.2s, 194063 effective words/s\n",
      "INFO - 18:59:10: EPOCH 24 - PROGRESS: at 23.65% examples, 186880 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:11: EPOCH 24 - PROGRESS: at 52.66% examples, 195435 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:12: EPOCH 24 - PROGRESS: at 77.68% examples, 195018 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:13: EPOCH 24 - PROGRESS: at 98.94% examples, 194964 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 18:59:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:13: EPOCH - 24 : training on 1091998 raw words (807932 effective words) took 4.1s, 195366 effective words/s\n",
      "INFO - 18:59:14: EPOCH 25 - PROGRESS: at 24.75% examples, 197413 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:15: EPOCH 25 - PROGRESS: at 53.83% examples, 196780 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:16: EPOCH 25 - PROGRESS: at 79.50% examples, 201150 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:17: EPOCH - 25 : training on 1091998 raw words (807848 effective words) took 4.1s, 199175 effective words/s\n",
      "INFO - 18:59:18: EPOCH 26 - PROGRESS: at 22.33% examples, 175907 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:19: EPOCH 26 - PROGRESS: at 50.33% examples, 185266 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:20: EPOCH 26 - PROGRESS: at 76.26% examples, 189379 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:21: EPOCH 26 - PROGRESS: at 97.24% examples, 190681 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 18:59:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:21: EPOCH - 26 : training on 1091998 raw words (807997 effective words) took 4.2s, 192014 effective words/s\n",
      "INFO - 18:59:22: EPOCH 27 - PROGRESS: at 22.33% examples, 183323 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:23: EPOCH 27 - PROGRESS: at 51.57% examples, 191777 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:24: EPOCH 27 - PROGRESS: at 77.68% examples, 196249 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:25: EPOCH 27 - PROGRESS: at 99.16% examples, 196275 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 18:59:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:25: EPOCH - 27 : training on 1091998 raw words (807793 effective words) took 4.1s, 197285 effective words/s\n",
      "INFO - 18:59:27: EPOCH 28 - PROGRESS: at 25.99% examples, 193447 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:28: EPOCH 28 - PROGRESS: at 55.84% examples, 202492 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:29: EPOCH 28 - PROGRESS: at 80.02% examples, 200772 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:29: EPOCH - 28 : training on 1091998 raw words (808518 effective words) took 4.0s, 202454 effective words/s\n",
      "INFO - 18:59:31: EPOCH 29 - PROGRESS: at 25.99% examples, 192562 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:32: EPOCH 29 - PROGRESS: at 55.84% examples, 201735 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:33: EPOCH 29 - PROGRESS: at 79.50% examples, 199672 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:34: EPOCH - 29 : training on 1091998 raw words (807948 effective words) took 4.1s, 197479 effective words/s\n",
      "INFO - 18:59:35: EPOCH 30 - PROGRESS: at 25.99% examples, 193221 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:36: EPOCH 30 - PROGRESS: at 57.10% examples, 200493 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:37: EPOCH 30 - PROGRESS: at 81.78% examples, 203782 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:38: EPOCH - 30 : training on 1091998 raw words (807842 effective words) took 4.0s, 202122 effective words/s\n",
      "INFO - 18:59:39: EPOCH 31 - PROGRESS: at 24.75% examples, 197981 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:40: EPOCH 31 - PROGRESS: at 53.83% examples, 197619 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:41: EPOCH 31 - PROGRESS: at 78.99% examples, 200160 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:42: EPOCH - 31 : training on 1091998 raw words (807780 effective words) took 4.0s, 201137 effective words/s\n",
      "INFO - 18:59:43: EPOCH 32 - PROGRESS: at 25.99% examples, 192868 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:44: EPOCH 32 - PROGRESS: at 55.84% examples, 202830 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:45: EPOCH 32 - PROGRESS: at 79.50% examples, 201681 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:46: EPOCH - 32 : training on 1091998 raw words (808068 effective words) took 4.0s, 200141 effective words/s\n",
      "INFO - 18:59:47: EPOCH 33 - PROGRESS: at 22.33% examples, 183911 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:48: EPOCH 33 - PROGRESS: at 53.83% examples, 195638 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:49: EPOCH 33 - PROGRESS: at 79.50% examples, 200719 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:50: EPOCH - 33 : training on 1091998 raw words (807856 effective words) took 4.0s, 201675 effective words/s\n",
      "INFO - 18:59:51: EPOCH 34 - PROGRESS: at 25.99% examples, 194357 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:52: EPOCH 34 - PROGRESS: at 55.84% examples, 203414 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:53: EPOCH 34 - PROGRESS: at 78.99% examples, 199379 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:54: EPOCH - 34 : training on 1091998 raw words (808244 effective words) took 4.1s, 199042 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:59:55: EPOCH 35 - PROGRESS: at 24.75% examples, 198076 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:56: EPOCH 35 - PROGRESS: at 53.83% examples, 199464 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 18:59:57: EPOCH 35 - PROGRESS: at 80.02% examples, 202535 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 18:59:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:59:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:59:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:59:58: EPOCH - 35 : training on 1091998 raw words (807959 effective words) took 3.9s, 204606 effective words/s\n",
      "INFO - 18:59:59: EPOCH 36 - PROGRESS: at 25.99% examples, 192482 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:00: EPOCH 36 - PROGRESS: at 55.84% examples, 201566 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:01: EPOCH 36 - PROGRESS: at 79.50% examples, 199412 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:02: EPOCH - 36 : training on 1091998 raw words (807818 effective words) took 4.1s, 199174 effective words/s\n",
      "INFO - 19:00:03: EPOCH 37 - PROGRESS: at 25.99% examples, 197883 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:04: EPOCH 37 - PROGRESS: at 57.10% examples, 206223 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:05: EPOCH 37 - PROGRESS: at 81.78% examples, 207348 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:06: EPOCH - 37 : training on 1091998 raw words (808104 effective words) took 4.0s, 204368 effective words/s\n",
      "INFO - 19:00:07: EPOCH 38 - PROGRESS: at 23.65% examples, 187982 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:08: EPOCH 38 - PROGRESS: at 53.83% examples, 195391 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:09: EPOCH 38 - PROGRESS: at 78.99% examples, 198195 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:10: EPOCH - 38 : training on 1091998 raw words (808507 effective words) took 4.0s, 201038 effective words/s\n",
      "INFO - 19:00:11: EPOCH 39 - PROGRESS: at 25.99% examples, 195223 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:12: EPOCH 39 - PROGRESS: at 57.10% examples, 201362 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:13: EPOCH 39 - PROGRESS: at 80.02% examples, 199277 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:14: EPOCH - 39 : training on 1091998 raw words (807656 effective words) took 4.0s, 200081 effective words/s\n",
      "INFO - 19:00:15: EPOCH 40 - PROGRESS: at 25.99% examples, 195680 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:16: EPOCH 40 - PROGRESS: at 57.10% examples, 204634 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:17: EPOCH 40 - PROGRESS: at 80.91% examples, 205021 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:18: EPOCH - 40 : training on 1091998 raw words (808119 effective words) took 3.9s, 205046 effective words/s\n",
      "INFO - 19:00:19: EPOCH 41 - PROGRESS: at 22.33% examples, 184197 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:20: EPOCH 41 - PROGRESS: at 50.33% examples, 188307 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:21: EPOCH 41 - PROGRESS: at 76.26% examples, 191556 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:22: EPOCH 41 - PROGRESS: at 97.24% examples, 192304 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 19:00:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:22: EPOCH - 41 : training on 1091998 raw words (808086 effective words) took 4.2s, 193612 effective words/s\n",
      "INFO - 19:00:23: EPOCH 42 - PROGRESS: at 24.75% examples, 196753 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:24: EPOCH 42 - PROGRESS: at 53.83% examples, 198982 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:25: EPOCH 42 - PROGRESS: at 80.02% examples, 202503 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:26: EPOCH - 42 : training on 1091998 raw words (807666 effective words) took 3.9s, 204598 effective words/s\n",
      "INFO - 19:00:27: EPOCH 43 - PROGRESS: at 25.99% examples, 194013 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:28: EPOCH 43 - PROGRESS: at 55.84% examples, 203011 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:29: EPOCH 43 - PROGRESS: at 80.02% examples, 201059 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:30: EPOCH - 43 : training on 1091998 raw words (808045 effective words) took 4.0s, 201713 effective words/s\n",
      "INFO - 19:00:31: EPOCH 44 - PROGRESS: at 22.33% examples, 184196 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:32: EPOCH 44 - PROGRESS: at 53.83% examples, 195482 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:33: EPOCH 44 - PROGRESS: at 80.02% examples, 199518 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:34: EPOCH - 44 : training on 1091998 raw words (807627 effective words) took 4.0s, 203089 effective words/s\n",
      "INFO - 19:00:35: EPOCH 45 - PROGRESS: at 25.99% examples, 199849 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:36: EPOCH 45 - PROGRESS: at 55.84% examples, 206009 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:37: EPOCH 45 - PROGRESS: at 80.02% examples, 203211 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:38: EPOCH - 45 : training on 1091998 raw words (807931 effective words) took 4.0s, 203031 effective words/s\n",
      "INFO - 19:00:39: EPOCH 46 - PROGRESS: at 23.65% examples, 189685 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:40: EPOCH 46 - PROGRESS: at 53.83% examples, 197764 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:41: EPOCH 46 - PROGRESS: at 80.02% examples, 201482 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:42: EPOCH - 46 : training on 1091998 raw words (807752 effective words) took 4.0s, 204445 effective words/s\n",
      "INFO - 19:00:43: EPOCH 47 - PROGRESS: at 25.99% examples, 192332 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:44: EPOCH 47 - PROGRESS: at 57.10% examples, 199654 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:45: EPOCH 47 - PROGRESS: at 80.91% examples, 201568 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:46: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:00:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:46: EPOCH - 47 : training on 1091998 raw words (807901 effective words) took 4.0s, 202334 effective words/s\n",
      "INFO - 19:00:47: EPOCH 48 - PROGRESS: at 25.99% examples, 191527 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:48: EPOCH 48 - PROGRESS: at 55.84% examples, 201929 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:49: EPOCH 48 - PROGRESS: at 80.02% examples, 198231 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:50: EPOCH - 48 : training on 1091998 raw words (807427 effective words) took 4.0s, 200628 effective words/s\n",
      "INFO - 19:00:51: EPOCH 49 - PROGRESS: at 24.75% examples, 197868 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:52: EPOCH 49 - PROGRESS: at 53.83% examples, 198171 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:00:53: EPOCH 49 - PROGRESS: at 79.50% examples, 202953 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:54: EPOCH - 49 : training on 1091998 raw words (807986 effective words) took 4.0s, 202515 effective words/s\n",
      "INFO - 19:00:55: EPOCH 50 - PROGRESS: at 24.75% examples, 198206 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:56: EPOCH 50 - PROGRESS: at 53.83% examples, 198874 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:57: EPOCH 50 - PROGRESS: at 78.33% examples, 198419 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:00:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:00:58: EPOCH 50 - PROGRESS: at 99.16% examples, 197637 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 19:00:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:00:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:00:58: EPOCH - 50 : training on 1091998 raw words (807501 effective words) took 4.1s, 198822 effective words/s\n",
      "INFO - 19:00:58: training on a 54599900 raw words (40396426 effective words) took 203.2s, 198809 effective words/s\n",
      "INFO - 19:00:58: saving Word2Vec object under WE_models/w2v_sg_p_100D, separately None\n",
      "INFO - 19:00:58: not storing attribute vectors_norm\n",
      "INFO - 19:00:58: not storing attribute cum_table\n",
      "INFO - 19:00:59: saved WE_models/w2v_sg_p_100D\n"
     ]
    }
   ],
   "source": [
    "sg_model_press = Word2Vec(sentences=sentences_press, size=wv_dim, sg=1, hs=1, min_count=1, iter=50)\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "sg_model_press.save('WE_models/w2v_sg_p_100D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:00:59: collecting all words and their counts\n",
      "INFO - 19:00:59: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 19:00:59: PROGRESS: at sentence #10000, processed 278773 words, keeping 18477 word types\n",
      "INFO - 19:00:59: PROGRESS: at sentence #20000, processed 531101 words, keeping 26360 word types\n",
      "INFO - 19:00:59: PROGRESS: at sentence #30000, processed 809820 words, keeping 31793 word types\n",
      "INFO - 19:00:59: collected 35706 word types from a corpus of 1091998 raw words and 38548 sentences\n",
      "INFO - 19:00:59: Loading a fresh vocabulary\n",
      "INFO - 19:00:59: effective_min_count=1 retains 35706 unique words (100% of original 35706, drops 0)\n",
      "INFO - 19:00:59: effective_min_count=1 leaves 1091998 word corpus (100% of original 1091998, drops 0)\n",
      "INFO - 19:00:59: deleting the raw counts dictionary of 35706 items\n",
      "INFO - 19:00:59: sample=0.001 downsamples 42 most-common words\n",
      "INFO - 19:00:59: downsampling leaves estimated 807950 word corpus (74.0% of prior 1091998)\n",
      "INFO - 19:00:59: constructing a huffman tree from 35706 words\n",
      "INFO - 19:01:06: built huffman tree with maximum node depth 20\n",
      "INFO - 19:01:06: estimated required memory for 35706 words and 100 dimensions: 67841400 bytes\n",
      "INFO - 19:01:06: resetting layer weights\n",
      "INFO - 19:01:06: training model with 3 workers on 35706 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO - 19:01:07: EPOCH 1 - PROGRESS: at 92.61% examples, 741625 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:07: EPOCH - 1 : training on 1091998 raw words (807918 effective words) took 1.1s, 750535 effective words/s\n",
      "INFO - 19:01:08: EPOCH 2 - PROGRESS: at 93.27% examples, 750373 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:08: EPOCH - 2 : training on 1091998 raw words (808006 effective words) took 1.1s, 748931 effective words/s\n",
      "INFO - 19:01:09: EPOCH 3 - PROGRESS: at 94.23% examples, 757603 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:09: EPOCH - 3 : training on 1091998 raw words (807923 effective words) took 1.1s, 758891 effective words/s\n",
      "INFO - 19:01:10: EPOCH 4 - PROGRESS: at 85.83% examples, 671163 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 19:01:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:10: EPOCH - 4 : training on 1091998 raw words (807968 effective words) took 1.2s, 674505 effective words/s\n",
      "INFO - 19:01:11: EPOCH 5 - PROGRESS: at 81.78% examples, 639806 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:12: EPOCH - 5 : training on 1091998 raw words (807844 effective words) took 1.3s, 644753 effective words/s\n",
      "INFO - 19:01:13: EPOCH 6 - PROGRESS: at 80.91% examples, 631530 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:13: EPOCH - 6 : training on 1091998 raw words (807573 effective words) took 1.3s, 642145 effective words/s\n",
      "INFO - 19:01:14: EPOCH 7 - PROGRESS: at 83.33% examples, 651473 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:14: EPOCH - 7 : training on 1091998 raw words (807332 effective words) took 1.2s, 663155 effective words/s\n",
      "INFO - 19:01:15: EPOCH 8 - PROGRESS: at 85.83% examples, 669699 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:15: EPOCH - 8 : training on 1091998 raw words (807403 effective words) took 1.2s, 676312 effective words/s\n",
      "INFO - 19:01:16: EPOCH 9 - PROGRESS: at 86.66% examples, 681275 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:16: EPOCH - 9 : training on 1091998 raw words (807364 effective words) took 1.2s, 683490 effective words/s\n",
      "INFO - 19:01:17: EPOCH 10 - PROGRESS: at 85.83% examples, 677941 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:18: EPOCH - 10 : training on 1091998 raw words (807785 effective words) took 1.2s, 687736 effective words/s\n",
      "INFO - 19:01:19: EPOCH 11 - PROGRESS: at 86.66% examples, 682729 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:19: EPOCH - 11 : training on 1091998 raw words (807860 effective words) took 1.2s, 687966 effective words/s\n",
      "INFO - 19:01:20: EPOCH 12 - PROGRESS: at 80.91% examples, 631207 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:20: EPOCH - 12 : training on 1091998 raw words (808411 effective words) took 1.3s, 619087 effective words/s\n",
      "INFO - 19:01:21: EPOCH 13 - PROGRESS: at 78.33% examples, 600467 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:21: EPOCH - 13 : training on 1091998 raw words (807549 effective words) took 1.3s, 611871 effective words/s\n",
      "INFO - 19:01:23: EPOCH 14 - PROGRESS: at 82.55% examples, 646153 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:23: EPOCH - 14 : training on 1091998 raw words (808494 effective words) took 1.2s, 653413 effective words/s\n",
      "INFO - 19:01:24: EPOCH 15 - PROGRESS: at 84.96% examples, 669082 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:24: EPOCH - 15 : training on 1091998 raw words (807611 effective words) took 1.2s, 678278 effective words/s\n",
      "INFO - 19:01:25: EPOCH 16 - PROGRESS: at 85.83% examples, 675238 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:25: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:01:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:25: EPOCH - 16 : training on 1091998 raw words (807636 effective words) took 1.2s, 681405 effective words/s\n",
      "INFO - 19:01:26: EPOCH 17 - PROGRESS: at 86.66% examples, 684904 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:26: EPOCH - 17 : training on 1091998 raw words (807798 effective words) took 1.2s, 694076 effective words/s\n",
      "INFO - 19:01:27: EPOCH 18 - PROGRESS: at 88.34% examples, 696411 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:27: EPOCH - 18 : training on 1091998 raw words (808377 effective words) took 1.2s, 701618 effective words/s\n",
      "INFO - 19:01:28: EPOCH 19 - PROGRESS: at 88.34% examples, 692959 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:01:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:29: EPOCH - 19 : training on 1091998 raw words (808584 effective words) took 1.2s, 701306 effective words/s\n",
      "INFO - 19:01:30: EPOCH 20 - PROGRESS: at 87.52% examples, 686095 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:30: EPOCH - 20 : training on 1091998 raw words (808018 effective words) took 1.2s, 691353 effective words/s\n",
      "INFO - 19:01:31: EPOCH 21 - PROGRESS: at 86.66% examples, 683975 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:31: EPOCH - 21 : training on 1091998 raw words (808053 effective words) took 1.2s, 691985 effective words/s\n",
      "INFO - 19:01:32: EPOCH 22 - PROGRESS: at 85.83% examples, 678272 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:32: EPOCH - 22 : training on 1091998 raw words (807811 effective words) took 1.2s, 689162 effective words/s\n",
      "INFO - 19:01:33: EPOCH 23 - PROGRESS: at 86.66% examples, 683117 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:01:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:33: EPOCH - 23 : training on 1091998 raw words (807656 effective words) took 1.2s, 687647 effective words/s\n",
      "INFO - 19:01:34: EPOCH 24 - PROGRESS: at 85.83% examples, 675056 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:35: EPOCH - 24 : training on 1091998 raw words (807898 effective words) took 1.2s, 685831 effective words/s\n",
      "INFO - 19:01:36: EPOCH 25 - PROGRESS: at 86.66% examples, 686383 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:36: EPOCH - 25 : training on 1091998 raw words (808237 effective words) took 1.2s, 692552 effective words/s\n",
      "INFO - 19:01:37: EPOCH 26 - PROGRESS: at 87.52% examples, 686382 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:37: EPOCH - 26 : training on 1091998 raw words (807441 effective words) took 1.2s, 694200 effective words/s\n",
      "INFO - 19:01:38: EPOCH 27 - PROGRESS: at 88.34% examples, 699669 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:01:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:38: EPOCH - 27 : training on 1091998 raw words (807758 effective words) took 1.1s, 710531 effective words/s\n",
      "INFO - 19:01:39: EPOCH 28 - PROGRESS: at 88.34% examples, 691735 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:39: EPOCH - 28 : training on 1091998 raw words (807873 effective words) took 1.2s, 702103 effective words/s\n",
      "INFO - 19:01:40: EPOCH 29 - PROGRESS: at 87.52% examples, 693186 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:40: EPOCH - 29 : training on 1091998 raw words (807821 effective words) took 1.2s, 694818 effective words/s\n",
      "INFO - 19:01:41: EPOCH 30 - PROGRESS: at 85.83% examples, 670549 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:42: EPOCH - 30 : training on 1091998 raw words (807817 effective words) took 1.2s, 680561 effective words/s\n",
      "INFO - 19:01:43: EPOCH 31 - PROGRESS: at 86.66% examples, 685535 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:43: EPOCH - 31 : training on 1091998 raw words (807844 effective words) took 1.2s, 690919 effective words/s\n",
      "INFO - 19:01:44: EPOCH 32 - PROGRESS: at 87.52% examples, 687826 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:44: EPOCH - 32 : training on 1091998 raw words (808444 effective words) took 1.2s, 695575 effective words/s\n",
      "INFO - 19:01:45: EPOCH 33 - PROGRESS: at 85.83% examples, 674578 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:45: EPOCH - 33 : training on 1091998 raw words (807829 effective words) took 1.2s, 684477 effective words/s\n",
      "INFO - 19:01:46: EPOCH 34 - PROGRESS: at 87.52% examples, 685716 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:46: EPOCH - 34 : training on 1091998 raw words (807846 effective words) took 1.2s, 684245 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:01:47: EPOCH 35 - PROGRESS: at 81.78% examples, 639058 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:48: EPOCH - 35 : training on 1091998 raw words (808042 effective words) took 1.3s, 641522 effective words/s\n",
      "INFO - 19:01:49: EPOCH 36 - PROGRESS: at 78.99% examples, 610405 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:01:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:49: EPOCH - 36 : training on 1091998 raw words (807788 effective words) took 1.3s, 631908 effective words/s\n",
      "INFO - 19:01:50: EPOCH 37 - PROGRESS: at 78.99% examples, 610279 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:50: EPOCH - 37 : training on 1091998 raw words (807806 effective words) took 1.3s, 615835 effective words/s\n",
      "INFO - 19:01:51: EPOCH 38 - PROGRESS: at 74.60% examples, 565686 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:52: EPOCH - 38 : training on 1091998 raw words (808016 effective words) took 1.4s, 582050 effective words/s\n",
      "INFO - 19:01:53: EPOCH 39 - PROGRESS: at 79.50% examples, 616529 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:01:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:53: EPOCH - 39 : training on 1091998 raw words (807562 effective words) took 1.3s, 626835 effective words/s\n",
      "INFO - 19:01:54: EPOCH 40 - PROGRESS: at 83.33% examples, 652259 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:01:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:54: EPOCH - 40 : training on 1091998 raw words (807855 effective words) took 1.2s, 659863 effective words/s\n",
      "INFO - 19:01:55: EPOCH 41 - PROGRESS: at 84.96% examples, 668491 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:55: EPOCH - 41 : training on 1091998 raw words (807772 effective words) took 1.2s, 679975 effective words/s\n",
      "INFO - 19:01:56: EPOCH 42 - PROGRESS: at 80.02% examples, 621199 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:57: EPOCH - 42 : training on 1091998 raw words (807611 effective words) took 1.3s, 628931 effective words/s\n",
      "INFO - 19:01:58: EPOCH 43 - PROGRESS: at 83.33% examples, 646972 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:01:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:58: EPOCH - 43 : training on 1091998 raw words (808360 effective words) took 1.2s, 659332 effective words/s\n",
      "INFO - 19:01:59: EPOCH 44 - PROGRESS: at 80.91% examples, 628914 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:01:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:01:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:01:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:01:59: EPOCH - 44 : training on 1091998 raw words (808204 effective words) took 1.3s, 645328 effective words/s\n",
      "INFO - 19:02:00: EPOCH 45 - PROGRESS: at 81.78% examples, 639364 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:02:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:02:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:02:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:02:00: EPOCH - 45 : training on 1091998 raw words (807592 effective words) took 1.2s, 654309 effective words/s\n",
      "INFO - 19:02:01: EPOCH 46 - PROGRESS: at 84.96% examples, 669091 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:02:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:02:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:02:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:02:01: EPOCH - 46 : training on 1091998 raw words (807983 effective words) took 1.2s, 675389 effective words/s\n",
      "INFO - 19:02:03: EPOCH 47 - PROGRESS: at 85.83% examples, 675333 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:02:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:02:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:02:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:02:03: EPOCH - 47 : training on 1091998 raw words (808209 effective words) took 1.2s, 686303 effective words/s\n",
      "INFO - 19:02:04: EPOCH 48 - PROGRESS: at 84.96% examples, 668066 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 19:02:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:02:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:02:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:02:04: EPOCH - 48 : training on 1091998 raw words (808474 effective words) took 1.2s, 672476 effective words/s\n",
      "INFO - 19:02:05: EPOCH 49 - PROGRESS: at 86.66% examples, 682477 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:02:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:02:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:02:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:02:05: EPOCH - 49 : training on 1091998 raw words (807387 effective words) took 1.2s, 686935 effective words/s\n",
      "INFO - 19:02:06: EPOCH 50 - PROGRESS: at 84.96% examples, 665456 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 19:02:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:02:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:02:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:02:06: EPOCH - 50 : training on 1091998 raw words (807639 effective words) took 1.2s, 672340 effective words/s\n",
      "INFO - 19:02:06: training on a 54599900 raw words (40394082 effective words) took 60.4s, 669013 effective words/s\n",
      "INFO - 19:02:06: saving Word2Vec object under WE_models/w2v_cbow_p_100D, separately None\n",
      "INFO - 19:02:06: not storing attribute vectors_norm\n",
      "INFO - 19:02:06: not storing attribute cum_table\n",
      "INFO - 19:02:07: saved WE_models/w2v_cbow_p_100D\n"
     ]
    }
   ],
   "source": [
    "cbow_model_press = Word2Vec(sentences=sentences_press, size=wv_dim, sg=0, hs=1, min_count=1, iter=50)\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "cbow_model_press.save('WE_models/w2v_cbow_p_100D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:32:33: resetting layer weights\n",
      "INFO - 16:32:43: collecting all words and their counts\n",
      "INFO - 16:32:43: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:32:43: collected 7826 word types from a corpus of 42967 raw words and 3091 sentences\n",
      "INFO - 16:32:43: Loading a fresh vocabulary\n",
      "INFO - 16:32:43: effective_min_count=1 retains 7826 unique words (100% of original 7826, drops 0)\n",
      "INFO - 16:32:43: effective_min_count=1 leaves 42967 word corpus (100% of original 42967, drops 0)\n",
      "INFO - 16:32:43: deleting the raw counts dictionary of 7826 items\n",
      "INFO - 16:32:43: sample=0.001 downsamples 34 most-common words\n",
      "INFO - 16:32:43: downsampling leaves estimated 31810 word corpus (74.0% of prior 42967)\n",
      "INFO - 16:32:43: constructing a huffman tree from 7826 words\n",
      "INFO - 16:32:43: built huffman tree with maximum node depth 16\n",
      "INFO - 16:32:44: estimated required memory for 7826 words, 61331 buckets and 100 dimensions: 41558608 bytes\n",
      "INFO - 16:32:44: resetting layer weights\n",
      "INFO - 16:32:51: training model with 3 workers on 7826 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "INFO - 16:32:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:51: EPOCH - 1 : training on 42967 raw words (31766 effective words) took 0.5s, 64847 effective words/s\n",
      "INFO - 16:32:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:51: EPOCH - 2 : training on 42967 raw words (31735 effective words) took 0.3s, 97923 effective words/s\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:52: EPOCH - 3 : training on 42967 raw words (31796 effective words) took 0.3s, 101092 effective words/s\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:52: EPOCH - 4 : training on 42967 raw words (31824 effective words) took 0.4s, 90434 effective words/s\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:52: EPOCH - 5 : training on 42967 raw words (31780 effective words) took 0.3s, 96411 effective words/s\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:53: EPOCH - 6 : training on 42967 raw words (31796 effective words) took 0.3s, 94976 effective words/s\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:53: EPOCH - 7 : training on 42967 raw words (31881 effective words) took 0.3s, 99673 effective words/s\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:53: EPOCH - 8 : training on 42967 raw words (31764 effective words) took 0.3s, 95162 effective words/s\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:54: EPOCH - 9 : training on 42967 raw words (31863 effective words) took 0.3s, 104833 effective words/s\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:54: EPOCH - 10 : training on 42967 raw words (31811 effective words) took 0.3s, 93558 effective words/s\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:54: EPOCH - 11 : training on 42967 raw words (31729 effective words) took 0.3s, 103944 effective words/s\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:55: EPOCH - 12 : training on 42967 raw words (31802 effective words) took 0.4s, 83861 effective words/s\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:55: EPOCH - 13 : training on 42967 raw words (31823 effective words) took 0.3s, 97898 effective words/s\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:55: EPOCH - 14 : training on 42967 raw words (31740 effective words) took 0.3s, 91852 effective words/s\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:56: EPOCH - 15 : training on 42967 raw words (31892 effective words) took 0.3s, 99460 effective words/s\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:56: EPOCH - 16 : training on 42967 raw words (31836 effective words) took 0.3s, 93037 effective words/s\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:56: EPOCH - 17 : training on 42967 raw words (31852 effective words) took 0.3s, 104881 effective words/s\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:57: EPOCH - 18 : training on 42967 raw words (31825 effective words) took 0.3s, 101274 effective words/s\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:57: EPOCH - 19 : training on 42967 raw words (31852 effective words) took 0.3s, 94746 effective words/s\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:58: EPOCH - 20 : training on 42967 raw words (31810 effective words) took 0.4s, 89351 effective words/s\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:32:58: EPOCH - 21 : training on 42967 raw words (31788 effective words) took 0.3s, 94613 effective words/s\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:58: EPOCH - 22 : training on 42967 raw words (31776 effective words) took 0.3s, 101202 effective words/s\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:59: EPOCH - 23 : training on 42967 raw words (31866 effective words) took 0.3s, 96815 effective words/s\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:59: EPOCH - 24 : training on 42967 raw words (31807 effective words) took 0.3s, 100760 effective words/s\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:32:59: EPOCH - 25 : training on 42967 raw words (31839 effective words) took 0.4s, 88386 effective words/s\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:32:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:00: EPOCH - 26 : training on 42967 raw words (31863 effective words) took 0.3s, 104129 effective words/s\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:00: EPOCH - 27 : training on 42967 raw words (31904 effective words) took 0.3s, 97852 effective words/s\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:00: EPOCH - 28 : training on 42967 raw words (31691 effective words) took 0.3s, 109715 effective words/s\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:00: EPOCH - 29 : training on 42967 raw words (31892 effective words) took 0.3s, 101139 effective words/s\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:01: EPOCH - 30 : training on 42967 raw words (31742 effective words) took 0.3s, 102502 effective words/s\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:01: EPOCH - 31 : training on 42967 raw words (31761 effective words) took 0.3s, 99978 effective words/s\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:01: EPOCH - 32 : training on 42967 raw words (31789 effective words) took 0.3s, 97524 effective words/s\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:02: EPOCH - 33 : training on 42967 raw words (31901 effective words) took 0.3s, 96092 effective words/s\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:02: EPOCH - 34 : training on 42967 raw words (31838 effective words) took 0.3s, 97148 effective words/s\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:03: EPOCH - 35 : training on 42967 raw words (31875 effective words) took 0.3s, 96865 effective words/s\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:03: EPOCH - 36 : training on 42967 raw words (31941 effective words) took 0.3s, 93000 effective words/s\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:03: EPOCH - 37 : training on 42967 raw words (31822 effective words) took 0.3s, 99734 effective words/s\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:04: EPOCH - 38 : training on 42967 raw words (31784 effective words) took 0.3s, 99354 effective words/s\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:04: EPOCH - 39 : training on 42967 raw words (31761 effective words) took 0.3s, 102585 effective words/s\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:04: EPOCH - 40 : training on 42967 raw words (31831 effective words) took 0.3s, 98724 effective words/s\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:05: EPOCH - 41 : training on 42967 raw words (31777 effective words) took 0.3s, 95162 effective words/s\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:05: EPOCH - 42 : training on 42967 raw words (31809 effective words) took 0.3s, 105539 effective words/s\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:05: EPOCH - 43 : training on 42967 raw words (31836 effective words) took 0.3s, 92761 effective words/s\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:06: EPOCH - 44 : training on 42967 raw words (31794 effective words) took 0.3s, 92819 effective words/s\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:33:06: EPOCH - 45 : training on 42967 raw words (31792 effective words) took 0.4s, 85713 effective words/s\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:06: EPOCH - 46 : training on 42967 raw words (31782 effective words) took 0.3s, 97657 effective words/s\n",
      "INFO - 16:33:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:07: EPOCH - 47 : training on 42967 raw words (31802 effective words) took 0.3s, 98404 effective words/s\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:07: EPOCH - 48 : training on 42967 raw words (31890 effective words) took 0.3s, 91275 effective words/s\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:07: EPOCH - 49 : training on 42967 raw words (31786 effective words) took 0.3s, 93103 effective words/s\n",
      "INFO - 16:33:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:33:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:33:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:33:08: EPOCH - 50 : training on 42967 raw words (31795 effective words) took 0.4s, 78381 effective words/s\n",
      "INFO - 16:33:08: training on a 2148350 raw words (1590711 effective words) took 17.2s, 92259 effective words/s\n",
      "INFO - 16:33:08: saving FastText object under WE_models/w2v_ft_p_100D, separately None\n",
      "INFO - 16:33:08: storing np array 'vectors_ngrams' to WE_models/w2v_ft_p_100D.wv.vectors_ngrams.npy\n",
      "INFO - 16:33:10: not storing attribute vectors_norm\n",
      "INFO - 16:33:10: not storing attribute vectors_vocab_norm\n",
      "INFO - 16:33:10: not storing attribute vectors_ngrams_norm\n",
      "INFO - 16:33:10: not storing attribute buckets_word\n",
      "INFO - 16:33:10: storing np array 'vectors_ngrams_lockf' to WE_models/w2v_ft_p_100D.trainables.vectors_ngrams_lockf.npy\n",
      "INFO - 16:33:13: saved WE_models/w2v_ft_p_100D\n"
     ]
    }
   ],
   "source": [
    "fasttext_model_press = gensim.models.fasttext.FastText(sentences=sentences_med, size=wv_dim, sg=0, hs=1, min_count=1, iter=50)\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "fasttext_model_press.save('WE_models/w2v_ft_p_100D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save keyed vectors\n",
    "# outfolder_path = \"../tp2/NeuroNLP2/experiments/models/we_models/\"\n",
    "\n",
    "# cbow_model.wv.save_word2vec_format(outfolder_path+\"w2v_cbow_100D.kv.bin\", binary=True)\n",
    "# cbow_model_press.wv.save_word2vec_format(outfolder_path+\"w2v_cbow_p_100D.kv\")\n",
    "# sg_model.wv.save_word2vec_format(outfolder_path+\"w2v_sg_100D.kv\")\n",
    "# sg_model_press.wv.save_word2vec_format(outfolder_path+\"w2v_sg_p_100D.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:57:52: storing 7826x100 projection weights into ../tp2/NeuroNLP2/experiments/models/we_models/w2v_cbow_100D.kv.bin\n",
      "INFO - 12:57:52: storing 35706x100 projection weights into ../tp2/NeuroNLP2/experiments/models/we_models/w2v_cbow_p_100D.kv\n",
      "INFO - 12:57:55: storing 7826x100 projection weights into ../tp2/NeuroNLP2/experiments/models/we_models/w2v_sg_100D.kv\n",
      "INFO - 12:57:55: storing 35706x100 projection weights into ../tp2/NeuroNLP2/experiments/models/we_models/w2v_sg_p_100D.kv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_path = outfolder_path+\"w2v_cbow_100D.kv.gz\"\n",
    "# m = gensim.models.KeyedVectors.load_word2vec_format(embedding_path, binary=False)\n",
    "# # word2vec = Word2Vec.load_word2vec_format(embedding_path, binary=False)\n",
    "# embedd_dim = m.vector_size\n",
    "# print(\"w2v\", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:10:29: loading Word2Vec object from WE_models/w2v_cbow_100D\n",
      "INFO - 22:10:30: loading wv recursively from WE_models/w2v_cbow_100D.wv.* with mmap=None\n",
      "INFO - 22:10:30: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:10:30: loading vocabulary recursively from WE_models/w2v_cbow_100D.vocabulary.* with mmap=None\n",
      "INFO - 22:10:30: loading trainables recursively from WE_models/w2v_cbow_100D.trainables.* with mmap=None\n",
      "INFO - 22:10:30: setting ignored attribute cum_table to None\n",
      "INFO - 22:10:30: loaded WE_models/w2v_cbow_100D\n",
      "INFO - 22:10:30: loading Word2Vec object from WE_models/w2v_cbow_p_100D\n",
      "INFO - 22:10:31: loading wv recursively from WE_models/w2v_cbow_p_100D.wv.* with mmap=None\n",
      "INFO - 22:10:31: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:10:31: loading vocabulary recursively from WE_models/w2v_cbow_p_100D.vocabulary.* with mmap=None\n",
      "INFO - 22:10:31: loading trainables recursively from WE_models/w2v_cbow_p_100D.trainables.* with mmap=None\n",
      "INFO - 22:10:31: setting ignored attribute cum_table to None\n",
      "INFO - 22:10:31: loaded WE_models/w2v_cbow_p_100D\n",
      "INFO - 22:10:31: loading Word2Vec object from WE_models/w2v_sg_100D\n",
      "INFO - 22:10:31: loading wv recursively from WE_models/w2v_sg_100D.wv.* with mmap=None\n",
      "INFO - 22:10:31: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:10:31: loading vocabulary recursively from WE_models/w2v_sg_100D.vocabulary.* with mmap=None\n",
      "INFO - 22:10:31: loading trainables recursively from WE_models/w2v_sg_100D.trainables.* with mmap=None\n",
      "INFO - 22:10:31: setting ignored attribute cum_table to None\n",
      "INFO - 22:10:31: loaded WE_models/w2v_sg_100D\n",
      "INFO - 22:10:31: loading Word2Vec object from WE_models/w2v_sg_p_100D\n",
      "INFO - 22:10:32: loading wv recursively from WE_models/w2v_sg_p_100D.wv.* with mmap=None\n",
      "INFO - 22:10:32: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:10:32: loading vocabulary recursively from WE_models/w2v_sg_p_100D.vocabulary.* with mmap=None\n",
      "INFO - 22:10:32: loading trainables recursively from WE_models/w2v_sg_p_100D.trainables.* with mmap=None\n",
      "INFO - 22:10:32: setting ignored attribute cum_table to None\n",
      "INFO - 22:10:32: loaded WE_models/w2v_sg_p_100D\n",
      "INFO - 22:10:32: loading Word2Vec object from WE_models/w2v_ft_100D\n",
      "INFO - 22:10:32: loading wv recursively from WE_models/w2v_ft_100D.wv.* with mmap=None\n",
      "INFO - 22:10:32: loading vectors_ngrams from WE_models/w2v_ft_100D.wv.vectors_ngrams.npy with mmap=None\n",
      "INFO - 22:10:33: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:10:33: setting ignored attribute vectors_vocab_norm to None\n",
      "INFO - 22:10:33: setting ignored attribute vectors_ngrams_norm to None\n",
      "INFO - 22:10:33: setting ignored attribute buckets_word to None\n",
      "INFO - 22:10:33: loading vocabulary recursively from WE_models/w2v_ft_100D.vocabulary.* with mmap=None\n",
      "INFO - 22:10:33: loading trainables recursively from WE_models/w2v_ft_100D.trainables.* with mmap=None\n",
      "INFO - 22:10:33: loading vectors_ngrams_lockf from WE_models/w2v_ft_100D.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "INFO - 22:10:34: loaded WE_models/w2v_ft_100D\n",
      "INFO - 22:10:34: loading Word2Vec object from WE_models/w2v_ft_p_100D\n",
      "INFO - 22:10:34: loading wv recursively from WE_models/w2v_ft_p_100D.wv.* with mmap=None\n",
      "INFO - 22:10:34: loading vectors_ngrams from WE_models/w2v_ft_p_100D.wv.vectors_ngrams.npy with mmap=None\n",
      "INFO - 22:10:35: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:10:35: setting ignored attribute vectors_vocab_norm to None\n",
      "INFO - 22:10:35: setting ignored attribute vectors_ngrams_norm to None\n",
      "INFO - 22:10:35: setting ignored attribute buckets_word to None\n",
      "INFO - 22:10:35: loading vocabulary recursively from WE_models/w2v_ft_p_100D.vocabulary.* with mmap=None\n",
      "INFO - 22:10:35: loading trainables recursively from WE_models/w2v_ft_p_100D.trainables.* with mmap=None\n",
      "INFO - 22:10:35: loading vectors_ngrams_lockf from WE_models/w2v_ft_p_100D.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "INFO - 22:10:36: loaded WE_models/w2v_ft_p_100D\n"
     ]
    }
   ],
   "source": [
    "# cbow med\n",
    "cbow_model = Word2Vec.load('WE_models/w2v_cbow_100D')\n",
    "\n",
    "# cbow press\n",
    "cbow_model_press = Word2Vec.load('WE_models/w2v_cbow_p_100D')\n",
    "\n",
    "# sg med\n",
    "sg_model = Word2Vec.load('WE_models/w2v_sg_100D')\n",
    "\n",
    "# sg press\n",
    "sg_model_press = Word2Vec.load('WE_models/w2v_sg_p_100D')\n",
    "\n",
    "# fasttext med\n",
    "fasttext_model = Word2Vec.load('WE_models/w2v_ft_100D')\n",
    "\n",
    "# fasttext press\n",
    "fasttext_model_press = Word2Vec.load('WE_models/w2v_ft_p_100D')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model.save('WE_models/w2v_cbow_100D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:36:03: saving Word2VecKeyedVectors object under w2v_ft_100D.kv, separately None\n",
      "INFO - 22:36:03: not storing attribute vectors_norm\n",
      "INFO - 22:36:03: saved w2v_ft_100D.kv\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "word_vectors = cbow_model.wv\n",
    "word_vectors.save('w2v_ft_100D.kv')\n",
    "# KeyedVectors.load('vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mots candidats: patient, traitement, maladie, solution, jaune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des embeddings entrainés sur le même corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tester l'impact des approches (skipgram, cbow, fasttext) sur le résultats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "R´ecup´erer les voisins s´emantiques d’un mot\n",
    "model.wv.most similar(\"cat\")\n",
    "model.wv.most similar(\"cat\", topn=5)\n",
    "Rang d’un voisin donn´e pour un mot donn´e\n",
    "model.wv.rank(\"cat\", \"dog\")\n",
    "Calcul du score cosinus entre 2 mots\n",
    "model.wv.similarity(\"cat\", \"dog\")\n",
    "Analogies\n",
    "model.wv.most similar(positive=[\"woman\", \"king\"],\n",
    "negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Med Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('montrez', 0.6341592073440552),\n",
       " ('alerte', 0.61311936378479),\n",
       " ('cette', 0.6056946516036987),\n",
       " ('aptitude', 0.6043680906295776),\n",
       " ('souffre', 0.5990329384803772),\n",
       " ('carte', 0.5902701616287231),\n",
       " ('existante', 0.5838398337364197),\n",
       " ('conserviez', 0.5823631882667542),\n",
       " ('determiner', 0.5807697176933289),\n",
       " ('speciale', 0.5757887959480286)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alerte', 0.5416854619979858),\n",
       " ('medicament', 0.48750820755958557),\n",
       " ('etre', 0.4820495843887329),\n",
       " ('vous', 0.47345608472824097),\n",
       " ('prialt', 0.4709300994873047),\n",
       " ('qu', 0.4424228370189667),\n",
       " ('ils', 0.44230562448501587),\n",
       " ('soient', 0.4418569803237915),\n",
       " ('recevoir', 0.43952542543411255),\n",
       " ('urinaire', 0.4368218779563904)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:12:52: precomputing L2-norms of word weight vectors\n",
      "INFO - 22:12:52: precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('patiente', 0.8910520076751709),\n",
       " ('patients', 0.8098596334457397),\n",
       " ('parvient', 0.70711350440979),\n",
       " ('soient', 0.6613249778747559),\n",
       " ('aient', 0.6562939286231995),\n",
       " ('conscient', 0.6520581245422363),\n",
       " ('maintient', 0.6402011513710022),\n",
       " ('gradient', 0.6337660551071167),\n",
       " ('recevaient', 0.630927562713623),\n",
       " ('emportent', 0.6303962469100952)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('par', 0.6101565957069397),\n",
       " ('instaure', 0.5734601616859436),\n",
       " ('traites', 0.5320462584495544),\n",
       " ('gynecomasties', 0.5252437591552734),\n",
       " ('commencer', 0.5163739919662476),\n",
       " ('significatifs', 0.49864545464515686),\n",
       " ('habitue', 0.49060970544815063),\n",
       " ('avant', 0.48643964529037476),\n",
       " ('reevaluer', 0.48350077867507935),\n",
       " ('experimente', 0.4764772951602936)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consequent', 0.48196345567703247),\n",
       " ('medecin', 0.47441399097442627),\n",
       " ('tasmar', 0.4606321454048157),\n",
       " ('cours', 0.4571623206138611),\n",
       " ('diagnostic', 0.42869091033935547),\n",
       " ('vih', 0.4021696448326111),\n",
       " ('instauration', 0.3941475749015808),\n",
       " ('chimiotherapie', 0.3862146735191345),\n",
       " ('traites', 0.38262954354286194),\n",
       " ('placebo', 0.37556999921798706)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('traitment', 0.9169358015060425),\n",
       " ('taaitement', 0.9022010564804077),\n",
       " ('evitement', 0.8458164930343628),\n",
       " ('traitements', 0.8318533897399902),\n",
       " ('allaitement', 0.8137481212615967),\n",
       " ('etroitement', 0.8132373094558716),\n",
       " ('correctement', 0.8128017783164978),\n",
       " ('recrutement', 0.8127676248550415),\n",
       " ('lentement', 0.8048897981643677),\n",
       " ('hautement', 0.7931801080703735)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maladie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parkinson', 0.6782119274139404),\n",
       " ('crohn', 0.6094835996627808),\n",
       " ('basedow', 0.588508129119873),\n",
       " ('vraie', 0.5692818760871887),\n",
       " ('idiopathique', 0.5633712410926819),\n",
       " ('hirsprung', 0.5631192922592163),\n",
       " ('recklinghausen', 0.5493713021278381),\n",
       " ('avance', 0.5429551601409912),\n",
       " ('fluctuations', 0.5345396399497986),\n",
       " ('bignami', 0.5342870950698853)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chronique', 0.47794264554977417),\n",
       " ('sida', 0.4588952660560608),\n",
       " ('affection', 0.4492846131324768),\n",
       " ('aigue', 0.44460824131965637),\n",
       " ('type', 0.44380277395248413),\n",
       " ('inhibition', 0.4374990165233612),\n",
       " ('pancreatite', 0.4326436519622803),\n",
       " ('sep', 0.429707407951355),\n",
       " ('nombre', 0.42318040132522583),\n",
       " ('douleurs', 0.4157176911830902)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('malade', 0.8126126527786255),\n",
       " ('maladies', 0.7736676335334778),\n",
       " ('amantadie', 0.7351871728897095),\n",
       " ('maldi', 0.7070671319961548),\n",
       " ('maltraitance', 0.6765587329864502),\n",
       " ('malaise', 0.6686583757400513),\n",
       " ('malades', 0.5842608213424683),\n",
       " ('malt', 0.5831559896469116),\n",
       " ('malgre', 0.5820046663284302),\n",
       " ('revelateurs', 0.5727584362030029)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contient', 0.7087570428848267),\n",
       " ('diluer', 0.685350775718689),\n",
       " ('ml', 0.6653280854225159),\n",
       " ('perfusable', 0.6614526510238647),\n",
       " ('injectable', 0.6604785919189453),\n",
       " ('buvable', 0.6535659432411194),\n",
       " ('reconstituee', 0.650699257850647),\n",
       " ('dosee', 0.6494290828704834),\n",
       " ('microgrammes', 0.6386682987213135),\n",
       " ('ajoutez', 0.6250247955322266)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', 0.620438814163208),\n",
       " ('preparation', 0.6042649745941162),\n",
       " ('lepirudine', 0.5789499878883362),\n",
       " ('vitesse', 0.5654317140579224),\n",
       " ('pompe', 0.546770453453064),\n",
       " ('instructions', 0.5461287498474121),\n",
       " ('chaque', 0.5169966220855713),\n",
       " ('bolus', 0.49907243251800537),\n",
       " ('manipulation', 0.4925003945827484),\n",
       " ('flacon', 0.47363755106925964)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dissolution', 0.9642202258110046),\n",
       " ('dilution', 0.8752856850624084),\n",
       " ('pollution', 0.8713375329971313),\n",
       " ('reconstitution', 0.8186519742012024),\n",
       " ('execution', 0.8122972846031189),\n",
       " ('evolution', 0.8111284375190735),\n",
       " ('constitution', 0.8108775615692139),\n",
       " ('substitution', 0.8104674816131592),\n",
       " ('institution', 0.801041841506958),\n",
       " ('microdeletion', 0.7897745370864868)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pale', 0.7044824361801147),\n",
       " ('orange', 0.64982008934021),\n",
       " ('anormale', 0.6196465492248535),\n",
       " ('calotermes', 0.6084887385368347),\n",
       " ('flavicollis', 0.6013085246086121),\n",
       " ('hexagonaux', 0.5832014083862305),\n",
       " ('fabr', 0.579716145992279),\n",
       " ('incolore', 0.5791885852813721),\n",
       " ('navet', 0.5746554136276245),\n",
       " ('mosaique', 0.5725802779197693)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oxyde', 0.6426818370819092),\n",
       " ('131', 0.6139801740646362),\n",
       " ('jugo', 0.604779064655304),\n",
       " ('commissuro', 0.59688800573349),\n",
       " ('tachetes', 0.573742151260376),\n",
       " ('spontane', 0.5644837021827698),\n",
       " ('intestine', 0.5587036609649658),\n",
       " ('scintigraphies', 0.5523563623428345),\n",
       " ('histidine', 0.5474417209625244),\n",
       " ('physiquement', 0.5420677065849304)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lane', 0.6069298982620239),\n",
       " ('rouge', 0.5857460498809814),\n",
       " ('jaunisse', 0.5671855807304382),\n",
       " ('oxyde', 0.5602437853813171),\n",
       " ('triacetine', 0.557691216468811),\n",
       " ('hippocrate', 0.5483880043029785),\n",
       " ('rougeole', 0.5414905548095703),\n",
       " ('titane', 0.5408645868301392),\n",
       " ('dioxyde', 0.538782000541687),\n",
       " ('fiable', 0.5368438959121704)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Press Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:09:29: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cancereux', 0.5618505477905273),\n",
       " ('soignant', 0.5616238117218018),\n",
       " ('algorithmes', 0.531581699848175),\n",
       " ('hospitalise', 0.5090411901473999),\n",
       " ('statistiquement', 0.5049449801445007),\n",
       " ('malade', 0.4799725413322449),\n",
       " ('161', 0.47708266973495483),\n",
       " ('insupportables', 0.4752955436706543),\n",
       " ('ricane', 0.4677731692790985),\n",
       " ('humble', 0.4670393168926239)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:09:29: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('soignant', 0.43600648641586304),\n",
       " ('sex', 0.4158717095851898),\n",
       " ('tourcoing', 0.390976220369339),\n",
       " ('garcon', 0.3902435898780823),\n",
       " ('mariees', 0.37666457891464233),\n",
       " ('lannes', 0.3766070008277893),\n",
       " ('lavage', 0.37340131402015686),\n",
       " ('produit', 0.3685552775859833),\n",
       " ('bouquin', 0.3608214557170868),\n",
       " ('fou', 0.3519297242164612)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:14:26: precomputing L2-norms of word weight vectors\n",
      "INFO - 22:14:26: precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('patiente', 0.887475311756134),\n",
       " ('patients', 0.8198944926261902),\n",
       " ('parvient', 0.7211000323295593),\n",
       " ('soient', 0.6863570809364319),\n",
       " ('aient', 0.6725202202796936),\n",
       " ('maintient', 0.6641037464141846),\n",
       " ('conscient', 0.6631748080253601),\n",
       " ('gradient', 0.6533800363540649),\n",
       " ('recevaient', 0.6456177234649658),\n",
       " ('presentaient', 0.6199854612350464)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model_press.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exciter', 0.5309458374977112),\n",
       " ('protegeait', 0.5193228125572205),\n",
       " ('medicamenteux', 0.5061299800872803),\n",
       " ('sida', 0.5061064958572388),\n",
       " ('mineur', 0.49692875146865845),\n",
       " ('antidouleur', 0.4894629120826721),\n",
       " ('generateurs', 0.48877307772636414),\n",
       " ('prevention', 0.4872512221336365),\n",
       " ('decentralise', 0.4819537401199341),\n",
       " ('fondamentales', 0.4691838026046753)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bilingues', 0.408133864402771),\n",
       " ('terrorisme', 0.40106064081192017),\n",
       " ('cout', 0.39213138818740845),\n",
       " ('soutien', 0.3916090726852417),\n",
       " ('renforcement', 0.38820207118988037),\n",
       " ('pacte', 0.37954139709472656),\n",
       " ('nom', 0.3591303825378418),\n",
       " ('sida', 0.35791391134262085),\n",
       " ('dictateur', 0.35676631331443787),\n",
       " ('minimum', 0.35663390159606934)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('traitment', 0.9169358015060425),\n",
       " ('taaitement', 0.9022010564804077),\n",
       " ('evitement', 0.8458164930343628),\n",
       " ('traitements', 0.8318533897399902),\n",
       " ('allaitement', 0.8137481212615967),\n",
       " ('etroitement', 0.8132373094558716),\n",
       " ('correctement', 0.8128017783164978),\n",
       " ('recrutement', 0.8127676248550415),\n",
       " ('lentement', 0.8048897981643677),\n",
       " ('hautement', 0.7931801080703735)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maladie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pneumopathie', 0.6438268423080444),\n",
       " ('epidemie', 0.6329473257064819),\n",
       " ('virale', 0.6205798387527466),\n",
       " ('succombent', 0.5800681114196777),\n",
       " ('161', 0.5724583268165588),\n",
       " ('atypique', 0.5670432448387146),\n",
       " ('neurologique', 0.5660151243209839),\n",
       " ('grippe', 0.564203679561615),\n",
       " ('succomber', 0.5592811107635498),\n",
       " ('transmissible', 0.5576953887939453)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('responsabilite', 0.46167290210723877),\n",
       " ('improvisation', 0.4057588577270508),\n",
       " ('fonction', 0.3996735215187073),\n",
       " ('violence', 0.398216187953949),\n",
       " ('gouvernant', 0.39815330505371094),\n",
       " ('situation', 0.395330011844635),\n",
       " ('peste', 0.3952723741531372),\n",
       " ('chambre', 0.39341360330581665),\n",
       " ('recourt', 0.38351255655288696),\n",
       " ('18000', 0.38294586539268494)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('malade', 0.8126126527786255),\n",
       " ('maladies', 0.7736676335334778),\n",
       " ('amantadie', 0.7351871728897095),\n",
       " ('maldi', 0.7070671319961548),\n",
       " ('maltraitance', 0.6765587329864502),\n",
       " ('malaise', 0.6686583757400513),\n",
       " ('malades', 0.5842608213424683),\n",
       " ('malt', 0.5831559896469116),\n",
       " ('malgre', 0.5820046663284302),\n",
       " ('revelateurs', 0.5727584362030029)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('garantissant', 0.6696721911430359),\n",
       " ('pacifique', 0.5904736518859863),\n",
       " ('sodium', 0.5753224492073059),\n",
       " ('mesure', 0.5710676908493042),\n",
       " ('lancinant', 0.5706446766853333),\n",
       " ('consensuelle', 0.5557705163955688),\n",
       " ('cochonneries', 0.5396841168403625),\n",
       " ('amelioree', 0.5380747318267822),\n",
       " ('constructif', 0.5126892328262329),\n",
       " ('prealable', 0.5051695108413696)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reponse', 0.5314513444900513),\n",
       " ('opportunite', 0.4558674693107605),\n",
       " ('mesure', 0.44619232416152954),\n",
       " ('facon', 0.4405023753643036),\n",
       " ('demarche', 0.43324095010757446),\n",
       " ('alternative', 0.43216174840927124),\n",
       " ('expertise', 0.4316878020763397),\n",
       " ('visite', 0.4223451018333435),\n",
       " ('mission', 0.4215881824493408),\n",
       " ('idee', 0.4190034866333008)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dissolution', 0.9642202258110046),\n",
       " ('dilution', 0.8752856850624084),\n",
       " ('pollution', 0.8713375329971313),\n",
       " ('reconstitution', 0.8186519742012024),\n",
       " ('execution', 0.8122972846031189),\n",
       " ('evolution', 0.8111284375190735),\n",
       " ('constitution', 0.8108775615692139),\n",
       " ('substitution', 0.8104674816131592),\n",
       " ('institution', 0.801041841506958),\n",
       " ('microdeletion', 0.7897745370864868)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maillot', 0.7700062990188599),\n",
       " ('pois', 0.6189182996749878),\n",
       " ('390', 0.606795072555542),\n",
       " ('lachhab', 0.5962772965431213),\n",
       " ('pena', 0.5697344541549683),\n",
       " ('decaleront', 0.5682517290115356),\n",
       " ('bradeley', 0.55889892578125),\n",
       " ('bradley', 0.5236215591430664),\n",
       " ('metal', 0.5199170112609863),\n",
       " ('endosse', 0.5177762508392334)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ouariour', 0.49236369132995605),\n",
       " ('souad', 0.4485026001930237),\n",
       " ('lachhab', 0.4437779188156128),\n",
       " ('aquitain', 0.42470255494117737),\n",
       " ('antonio', 0.4102002680301666),\n",
       " ('vedrine', 0.40616124868392944),\n",
       " ('empare', 0.40327855944633484),\n",
       " ('pois', 0.4008446931838989),\n",
       " ('muette', 0.39541828632354736),\n",
       " ('azur', 0.3890635371208191)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lane', 0.6069298982620239),\n",
       " ('rouge', 0.5857460498809814),\n",
       " ('jaunisse', 0.5671855807304382),\n",
       " ('oxyde', 0.5602437853813171),\n",
       " ('triacetine', 0.557691216468811),\n",
       " ('hippocrate', 0.5483880043029785),\n",
       " ('rougeole', 0.5414905548095703),\n",
       " ('titane', 0.5408645868301392),\n",
       " ('dioxyde', 0.538782000541687),\n",
       " ('fiable', 0.5368438959121704)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparer des embeddings (même approche) entrainés sur de corpus différents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tester l'impact de données (type et quantité) sur les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3091"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38548"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_press)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('montrez', 0.6341592073440552),\n",
       " ('alerte', 0.61311936378479),\n",
       " ('cette', 0.6056946516036987),\n",
       " ('aptitude', 0.6043680906295776),\n",
       " ('souffre', 0.5990329384803772),\n",
       " ('carte', 0.5902701616287231),\n",
       " ('existante', 0.5838398337364197),\n",
       " ('conserviez', 0.5823631882667542),\n",
       " ('determiner', 0.5807697176933289),\n",
       " ('speciale', 0.5757887959480286)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alerte', 0.5416854619979858),\n",
       " ('medicament', 0.48750820755958557),\n",
       " ('etre', 0.4820495843887329),\n",
       " ('vous', 0.47345608472824097),\n",
       " ('prialt', 0.4709300994873047),\n",
       " ('qu', 0.4424228370189667),\n",
       " ('ils', 0.44230562448501587),\n",
       " ('soient', 0.4418569803237915),\n",
       " ('recevoir', 0.43952542543411255),\n",
       " ('urinaire', 0.4368218779563904)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('par', 0.6101565957069397),\n",
       " ('instaure', 0.5734601616859436),\n",
       " ('traites', 0.5320462584495544),\n",
       " ('gynecomasties', 0.5252437591552734),\n",
       " ('commencer', 0.5163739919662476),\n",
       " ('significatifs', 0.49864545464515686),\n",
       " ('habitue', 0.49060970544815063),\n",
       " ('avant', 0.48643964529037476),\n",
       " ('reevaluer', 0.48350077867507935),\n",
       " ('experimente', 0.4764772951602936)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consequent', 0.48196345567703247),\n",
       " ('medecin', 0.47441399097442627),\n",
       " ('tasmar', 0.4606321454048157),\n",
       " ('cours', 0.4571623206138611),\n",
       " ('diagnostic', 0.42869091033935547),\n",
       " ('vih', 0.4021696448326111),\n",
       " ('instauration', 0.3941475749015808),\n",
       " ('chimiotherapie', 0.3862146735191345),\n",
       " ('traites', 0.38262954354286194),\n",
       " ('placebo', 0.37556999921798706)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maladie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parkinson', 0.6782119274139404),\n",
       " ('crohn', 0.6094835996627808),\n",
       " ('basedow', 0.588508129119873),\n",
       " ('vraie', 0.5692818760871887),\n",
       " ('idiopathique', 0.5633712410926819),\n",
       " ('hirsprung', 0.5631192922592163),\n",
       " ('recklinghausen', 0.5493713021278381),\n",
       " ('avance', 0.5429551601409912),\n",
       " ('fluctuations', 0.5345396399497986),\n",
       " ('bignami', 0.5342870950698853)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chronique', 0.47794264554977417),\n",
       " ('sida', 0.4588952660560608),\n",
       " ('affection', 0.4492846131324768),\n",
       " ('aigue', 0.44460824131965637),\n",
       " ('type', 0.44380277395248413),\n",
       " ('inhibition', 0.4374990165233612),\n",
       " ('pancreatite', 0.4326436519622803),\n",
       " ('sep', 0.429707407951355),\n",
       " ('nombre', 0.42318040132522583),\n",
       " ('douleurs', 0.4157176911830902)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contient', 0.7087570428848267),\n",
       " ('diluer', 0.685350775718689),\n",
       " ('ml', 0.6653280854225159),\n",
       " ('perfusable', 0.6614526510238647),\n",
       " ('injectable', 0.6604785919189453),\n",
       " ('buvable', 0.6535659432411194),\n",
       " ('reconstituee', 0.650699257850647),\n",
       " ('dosee', 0.6494290828704834),\n",
       " ('microgrammes', 0.6386682987213135),\n",
       " ('ajoutez', 0.6250247955322266)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', 0.620438814163208),\n",
       " ('preparation', 0.6042649745941162),\n",
       " ('lepirudine', 0.5789499878883362),\n",
       " ('vitesse', 0.5654317140579224),\n",
       " ('pompe', 0.546770453453064),\n",
       " ('instructions', 0.5461287498474121),\n",
       " ('chaque', 0.5169966220855713),\n",
       " ('bolus', 0.49907243251800537),\n",
       " ('manipulation', 0.4925003945827484),\n",
       " ('flacon', 0.47363755106925964)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pale', 0.7044824361801147),\n",
       " ('orange', 0.64982008934021),\n",
       " ('anormale', 0.6196465492248535),\n",
       " ('calotermes', 0.6084887385368347),\n",
       " ('flavicollis', 0.6013085246086121),\n",
       " ('hexagonaux', 0.5832014083862305),\n",
       " ('fabr', 0.579716145992279),\n",
       " ('incolore', 0.5791885852813721),\n",
       " ('navet', 0.5746554136276245),\n",
       " ('mosaique', 0.5725802779197693)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oxyde', 0.6426818370819092),\n",
       " ('131', 0.6139801740646362),\n",
       " ('jugo', 0.604779064655304),\n",
       " ('commissuro', 0.59688800573349),\n",
       " ('tachetes', 0.573742151260376),\n",
       " ('spontane', 0.5644837021827698),\n",
       " ('intestine', 0.5587036609649658),\n",
       " ('scintigraphies', 0.5523563623428345),\n",
       " ('histidine', 0.5474417209625244),\n",
       " ('physiquement', 0.5420677065849304)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_press.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alerte', 0.5416854619979858),\n",
       " ('medicament', 0.48750820755958557),\n",
       " ('etre', 0.4820495843887329),\n",
       " ('vous', 0.47345608472824097),\n",
       " ('prialt', 0.4709300994873047),\n",
       " ('qu', 0.4424228370189667),\n",
       " ('ils', 0.44230562448501587),\n",
       " ('soient', 0.4418569803237915),\n",
       " ('recevoir', 0.43952542543411255),\n",
       " ('urinaire', 0.4368218779563904)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soignant', 0.43600648641586304),\n",
       " ('sex', 0.4158717095851898),\n",
       " ('tourcoing', 0.390976220369339),\n",
       " ('garcon', 0.3902435898780823),\n",
       " ('mariees', 0.37666457891464233),\n",
       " ('lannes', 0.3766070008277893),\n",
       " ('lavage', 0.37340131402015686),\n",
       " ('produit', 0.3685552775859833),\n",
       " ('bouquin', 0.3608214557170868),\n",
       " ('fou', 0.3519297242164612)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consequent', 0.48196345567703247),\n",
       " ('medecin', 0.47441399097442627),\n",
       " ('tasmar', 0.4606321454048157),\n",
       " ('cours', 0.4571623206138611),\n",
       " ('diagnostic', 0.42869091033935547),\n",
       " ('vih', 0.4021696448326111),\n",
       " ('instauration', 0.3941475749015808),\n",
       " ('chimiotherapie', 0.3862146735191345),\n",
       " ('traites', 0.38262954354286194),\n",
       " ('placebo', 0.37556999921798706)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bilingues', 0.408133864402771),\n",
       " ('terrorisme', 0.40106064081192017),\n",
       " ('cout', 0.39213138818740845),\n",
       " ('soutien', 0.3916090726852417),\n",
       " ('renforcement', 0.38820207118988037),\n",
       " ('pacte', 0.37954139709472656),\n",
       " ('nom', 0.3591303825378418),\n",
       " ('sida', 0.35791391134262085),\n",
       " ('dictateur', 0.35676631331443787),\n",
       " ('minimum', 0.35663390159606934)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maladie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chronique', 0.47794264554977417),\n",
       " ('sida', 0.4588952660560608),\n",
       " ('affection', 0.4492846131324768),\n",
       " ('aigue', 0.44460824131965637),\n",
       " ('type', 0.44380277395248413),\n",
       " ('inhibition', 0.4374990165233612),\n",
       " ('pancreatite', 0.4326436519622803),\n",
       " ('sep', 0.429707407951355),\n",
       " ('nombre', 0.42318040132522583),\n",
       " ('douleurs', 0.4157176911830902)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('responsabilite', 0.46167290210723877),\n",
       " ('improvisation', 0.4057588577270508),\n",
       " ('fonction', 0.3996735215187073),\n",
       " ('violence', 0.398216187953949),\n",
       " ('gouvernant', 0.39815330505371094),\n",
       " ('situation', 0.395330011844635),\n",
       " ('peste', 0.3952723741531372),\n",
       " ('chambre', 0.39341360330581665),\n",
       " ('recourt', 0.38351255655288696),\n",
       " ('18000', 0.38294586539268494)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', 0.620438814163208),\n",
       " ('preparation', 0.6042649745941162),\n",
       " ('lepirudine', 0.5789499878883362),\n",
       " ('vitesse', 0.5654317140579224),\n",
       " ('pompe', 0.546770453453064),\n",
       " ('instructions', 0.5461287498474121),\n",
       " ('chaque', 0.5169966220855713),\n",
       " ('bolus', 0.49907243251800537),\n",
       " ('manipulation', 0.4925003945827484),\n",
       " ('flacon', 0.47363755106925964)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reponse', 0.5314513444900513),\n",
       " ('opportunite', 0.4558674693107605),\n",
       " ('mesure', 0.44619232416152954),\n",
       " ('facon', 0.4405023753643036),\n",
       " ('demarche', 0.43324095010757446),\n",
       " ('alternative', 0.43216174840927124),\n",
       " ('expertise', 0.4316878020763397),\n",
       " ('visite', 0.4223451018333435),\n",
       " ('mission', 0.4215881824493408),\n",
       " ('idee', 0.4190034866333008)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oxyde', 0.6426818370819092),\n",
       " ('131', 0.6139801740646362),\n",
       " ('jugo', 0.604779064655304),\n",
       " ('commissuro', 0.59688800573349),\n",
       " ('tachetes', 0.573742151260376),\n",
       " ('spontane', 0.5644837021827698),\n",
       " ('intestine', 0.5587036609649658),\n",
       " ('scintigraphies', 0.5523563623428345),\n",
       " ('histidine', 0.5474417209625244),\n",
       " ('physiquement', 0.5420677065849304)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ouariour', 0.49236369132995605),\n",
       " ('souad', 0.4485026001930237),\n",
       " ('lachhab', 0.4437779188156128),\n",
       " ('aquitain', 0.42470255494117737),\n",
       " ('antonio', 0.4102002680301666),\n",
       " ('vedrine', 0.40616124868392944),\n",
       " ('empare', 0.40327855944633484),\n",
       " ('pois', 0.4008446931838989),\n",
       " ('muette', 0.39541828632354736),\n",
       " ('azur', 0.3890635371208191)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model_press.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patiente', 0.8910520076751709),\n",
       " ('patients', 0.8098596334457397),\n",
       " ('parvient', 0.70711350440979),\n",
       " ('soient', 0.6613249778747559),\n",
       " ('aient', 0.6562939286231995),\n",
       " ('conscient', 0.6520581245422363),\n",
       " ('maintient', 0.6402011513710022),\n",
       " ('gradient', 0.6337660551071167),\n",
       " ('recevaient', 0.630927562713623),\n",
       " ('emportent', 0.6303962469100952)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patiente', 0.887475311756134),\n",
       " ('patients', 0.8198944926261902),\n",
       " ('parvient', 0.7211000323295593),\n",
       " ('soient', 0.6863570809364319),\n",
       " ('aient', 0.6725202202796936),\n",
       " ('maintient', 0.6641037464141846),\n",
       " ('conscient', 0.6631748080253601),\n",
       " ('gradient', 0.6533800363540649),\n",
       " ('recevaient', 0.6456177234649658),\n",
       " ('presentaient', 0.6199854612350464)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model_press.wv.most_similar(\"patient\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('traitment', 0.9169358015060425),\n",
       " ('taaitement', 0.9022010564804077),\n",
       " ('evitement', 0.8458164930343628),\n",
       " ('traitements', 0.8318533897399902),\n",
       " ('allaitement', 0.8137481212615967),\n",
       " ('etroitement', 0.8132373094558716),\n",
       " ('correctement', 0.8128017783164978),\n",
       " ('recrutement', 0.8127676248550415),\n",
       " ('lentement', 0.8048897981643677),\n",
       " ('hautement', 0.7931801080703735)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('traitment', 0.9091444611549377),\n",
       " ('taaitement', 0.9002687931060791),\n",
       " ('evitement', 0.8486114144325256),\n",
       " ('traitements', 0.840034008026123),\n",
       " ('etroitement', 0.8144130706787109),\n",
       " ('lentement', 0.8014160394668579),\n",
       " ('correctement', 0.800934910774231),\n",
       " ('allaitement', 0.7987581491470337),\n",
       " ('recrutement', 0.7970902323722839),\n",
       " ('hautement', 0.7945890426635742)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model_press.wv.most_similar(\"traitement\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maladie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('malade', 0.8126126527786255),\n",
       " ('maladies', 0.7736676335334778),\n",
       " ('amantadie', 0.7351871728897095),\n",
       " ('maldi', 0.7070671319961548),\n",
       " ('maltraitance', 0.6765587329864502),\n",
       " ('malaise', 0.6686583757400513),\n",
       " ('malades', 0.5842608213424683),\n",
       " ('malt', 0.5831559896469116),\n",
       " ('malgre', 0.5820046663284302),\n",
       " ('revelateurs', 0.5727584362030029)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('malade', 0.8307108879089355),\n",
       " ('maladies', 0.7987197637557983),\n",
       " ('amantadie', 0.753980278968811),\n",
       " ('maldi', 0.7389033436775208),\n",
       " ('maltraitance', 0.688429594039917),\n",
       " ('malaise', 0.6574214696884155),\n",
       " ('malt', 0.6551423072814941),\n",
       " ('malades', 0.6232011318206787),\n",
       " ('zie', 0.6144228577613831),\n",
       " ('malherbe', 0.5927330255508423)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model_press.wv.most_similar(\"maladie\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dissolution', 0.9642202258110046),\n",
       " ('dilution', 0.8752856850624084),\n",
       " ('pollution', 0.8713375329971313),\n",
       " ('reconstitution', 0.8186519742012024),\n",
       " ('execution', 0.8122972846031189),\n",
       " ('evolution', 0.8111284375190735),\n",
       " ('constitution', 0.8108775615692139),\n",
       " ('substitution', 0.8104674816131592),\n",
       " ('institution', 0.801041841506958),\n",
       " ('microdeletion', 0.7897745370864868)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dissolution', 0.9617725610733032),\n",
       " ('dilution', 0.8633571863174438),\n",
       " ('pollution', 0.8465920686721802),\n",
       " ('execution', 0.8038966655731201),\n",
       " ('evolution', 0.7997591495513916),\n",
       " ('constitution', 0.7981904149055481),\n",
       " ('reconstitution', 0.7939782738685608),\n",
       " ('substitution', 0.7934744954109192),\n",
       " ('preparation', 0.7868441939353943),\n",
       " ('institution', 0.7852331399917603)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model_press.wv.most_similar(\"solution\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lane', 0.6069298982620239),\n",
       " ('rouge', 0.5857460498809814),\n",
       " ('jaunisse', 0.5671855807304382),\n",
       " ('oxyde', 0.5602437853813171),\n",
       " ('triacetine', 0.557691216468811),\n",
       " ('hippocrate', 0.5483880043029785),\n",
       " ('rougeole', 0.5414905548095703),\n",
       " ('titane', 0.5408645868301392),\n",
       " ('dioxyde', 0.538782000541687),\n",
       " ('fiable', 0.5368438959121704)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oxyde', 0.5710400342941284),\n",
       " ('lane', 0.5590745210647583),\n",
       " ('triangle', 0.5448751449584961),\n",
       " ('ethylcellulose', 0.5370972156524658),\n",
       " ('auvergne', 0.5358014106750488),\n",
       " ('dioxyde', 0.532309889793396),\n",
       " ('cellulose', 0.5294350981712341),\n",
       " ('titane', 0.5276556015014648),\n",
       " ('jaunisse', 0.527066707611084),\n",
       " ('macrogol', 0.5252885222434998)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model_press.wv.most_similar(\"jaune\", topn=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "Calculer une metrique moyenne a partir des mots les plus similaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les mots les plus similaires calculés par Fasttext sont ceux s'ecrivant de maniere similaire au mot d'interet. Cela est du à la maniere dont Fasttext découpe les mots. Néanmoins ces mots souvent sont parfois sématiquement proches.\n",
    "\n",
    "On note qu'avec les methodes Skip gram et CBOW les mots apparaissant dans le contexte d'un mot d'interet sont bien predits. Cependant il y'a parfois des stops words parmi les mots les plus similaires. Ce qui suggere de faire un pre-traitement sur le texte pour de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
